<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Installing HDF on Ambari 2.7.1 + HDP 3.0.1]]></title>
    <url>%2Fposts%2F8c49992b.html</url>
    <content type="text"><![CDATA[Installed Ambari and HDP are prerequisites for installing HDF on HDP clusters. So, after that, you can follow steps below to complete installing HDF. Downloading installation package. Accessing the URL “https://docs.cloudera.com/HDPDocuments/HDF3/HDF-3.3.1/release-notes/content/hdf_repository_locations.html&quot; to acquire the download location for your specific OS type. Copy the location for “HDF Management Pack” and “HDF Repo” and download these resources on the host where you installed Ambari. Installing the HDF Management Pack. Stop all services on your Ambari. Stop your Ambari by command. 1ambari-server stop Run the command below on the node where the Ambari locates. 123ambari-server install-mpack \--mpack=/tmp/hdf-ambari-mpack-&lt;version&gt;.tar.gz \--verbose Start your Ambari. 1ambari-server start Adding HDF service to your HDP cluster. Unzipping your HDF installation package to your http server and ensure that you can access the HDF intallation resources. Signing in your Ambari by user admin and registering HDF-3.2. After that, you will notice a brand new stack component named “HDF-3.2.0.0” appearing on the page “Versions”. Following steps on Ambari Service Wizard to install NiFi. So now, you can install the component NiFi which contained in HDF by following steps on Ambari Service Wizard.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tutorial for Latest Ambari(2.7.1)]]></title>
    <url>%2Fposts%2F937f0590.html</url>
    <content type="text"><![CDATA[Before I start to install Ambari 2.7.1, I wrote some key information that I need to use later. Key Information hostname ip operating system RAM disk space cores of CPU package master01.ambari.com 192.168.110.210 CentOS-7-x86_64-Minimal-1611.iso 20g 100g 16 ambari-server slave01.ambari.com 192.168.110.211 CentOS-7-x86_64-Minimal-1611.iso 20g 100g 16 ambari-agent slave02.ambari.com 192.168.110.212 CentOS-7-x86_64-Minimal-1611.iso 20g 100g 16 ambari-agent VM Setting and CentOS7 InstallationVM SettingUpload the IOS fileBefore we start to create VMs and install CentOS7, we ought to upload CentOS7 installation file (using minimal version) to Datacenter in VMware Vsphere. Choose specific data node in your Datacenter, then upload files. Create VMs Visit and login vsphere Web Client. Create a folder for collecting all nodes. Create a VM node in specific folder that created in last step. Input the name of VM Choose computing resource. Choose storage node. Choose the compatibility of your VM. Choose the version of your VM. Input the number of your CPU cores, the memory size, the hard-drive size and the IOS file path of your CentOS7. CentOS7 Installation Power on the VM. Choose language, then click the button “Continue”. Start to update setting(changing date&amp;time, specifying installation destination, turning on the network). Specify the installation destination. Turn on the network or update the configuration of the network. Finally, update the password of user root. File PrepationWe can download requisite installation file from official website below. 12345678#Ambarihttps://docs.hortonworks.com/HDPDocuments/Ambari-2.7.1.0/bk_ambari-installation/content/ambari_repositories.html#HDPhttps://docs.hortonworks.com/HDPDocuments/Ambari-2.7.1.0/bk_ambari-installation/content/hdp_30_repositories.html#HDFhttps://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.2.0/release-notes/content/hdf_repository_locations.html Ambari installation file. ambari-2.7.1.0-centos7.tar.gz.tar HDP installation file. HDP-3.0.1.0-centos7-rpm.tar.gz.tar HDP-UTILS-1.1.0.22-centos7.tar.gz.tar HDF installation file. hdf-ambari-mpack-3.2.0.0-520.tar.gz.tar HDF-3.2.0.0-centos7-rpm.tar.gz.tar System PrepationBefore we start to install Ambari 2.7.1 cluster, some important steps we need to complete. Install some basic tools. 12yum -y install unzipyum -y install wget Change the Hostname on all cluster hosts. Edit the Host File on all cluster hosts. 1vi /etc/hosts Then append the content blow to the Host File (Do NOT delete the original contents). 123192.168.110.210 master01.ambari.com master01192.168.110.211 slave01.ambari.com slave01192.168.110.212 slave02.ambari.com slave02 Set the Hostname on all cluster hosts. master01.ambari.com: 1hostname master01.ambari.com slave01.ambari.com: 1hostname slave01.ambari.com slave02.ambari.com: 1hostname slave02.ambari.com Edit the Network Configuration File on all cluster hosts. 1vi /etc/sysconfig/network Then append the contents blow to the file. 12NETWORKING=yesHOSTNAME=&lt;fully.qualified.domain.name&gt; Test the hostname on all cluster hosts. 12hostnamehostname -f If the result above is incorrect, you need to repair until the result is that you expect. Set up Password-less SSH To have Ambari Server automatically install Ambari Agent on all your cluster hosts, you must set up password-less SSH connection between the Ambari Server host and all other hosts in the cluster. Generate public and private SSH keys on the Ambari Server host. 1ssh-keygen Using ssh command to copy master public key file to slave nodes. 12ssh-copy-id -i ~/.ssh/id_rsa.pub root@slave01.ambari.comssh-copy-id -i ~/.ssh/id_rsa.pub root@slave02.ambari.com Test connections between the Ambari Server and the other cluster hosts. 123ssh root@master01.ambari.comssh root@slave01.ambari.comssh root@slave02.ambari.com It is successful when you do not need to input any passwords. Enable NTP on all cluster hosts. 12yum install -y ntpsystemctl enable ntpd Disable SELinux on all cluster hosts. 1vi /etc/selinux/config Then change the value of SELINUX. 1SELINUX=disabled Disable iptables on all cluster hosts. For Ambari to communicate during installation, certain ports must be open and available. So, the easiest way to do this is to temporarily disable iptables. 12systemctl disable firewalldsystemctl stop firewalld Install Oracle JDK8 (Not JRE) and add JCE extension package on all cluster hosts. You can also install JDK8 including JCE automatically on the step that run command “amber-server setup” Download Oracle JDK8 installation file from Oracle. Download JCE installation file. 1http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html Install JDK8 firstlly on all cluster hosts. 1yum install [your jdk8 yum installation file] Unzip JCE file to “$JAVA_HOME/jre/lib/security/“ on all cluster hosts. 1unzip -o -j -q jce_policy-8.zip -d /usr/java/jdk1.8.0_201-amd64/jre/lib/security Reboot. JDK HOME 1/usr/java/jdk1.8.0_201-amd64/ Install Mysql Server on the Ambari Server host. Download and add the repository, then update. 123wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum update Install mysql-server and start the service. 12yum install mysql-serversystemctl start mysqld Change the password of the user ROOT (the default password is blank). 1mysql -uroot -p 12set password for 'root'@'localhost' = password('root');set password for 'root'@'master01.ambari.com' = password('root'); Create the database “ambari” and the user “ambari”. 1234567create database ambari;create user 'ambari'@'localhost' identified by 'ambari';create user 'ambari'@'master01.ambari.com' identified by 'ambari';grant all on ambari.* to 'ambari'@'localhost';grant all on ambari.* to 'ambari'@'master01.ambari.com'; Download mysql-connector-java.jar and put this jar to a specific folder on the Ambari Server host. 12345678wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jarmkdir /usr/share/javacp /root/mysql-connector-java-5.1.47.jar /usr/share/java/#JDBC Driver Path/usr/share/java/mysql-connector-java-5.1.47.jar Setting up a local repository with NO Internet access on the Ambari Server host. Install and start httpd. 12yum -y install httpdsystemctl start httpd Untar installation file to web server folder. 1234567mkdir /var/www/html/hdfmkdir /var/www/html/hdptar -xvf /root/ambari-2.7.1.0-centos7.tar.gz.tar -C /var/www/htmltar -xvf /root/HDP-3.0.1.0-centos7-rpm.tar.gz.tar -C /var/www/html/hdptar -xvf /root/HDP-UTILS-1.1.0.22-centos7.tar.gz.tar -C /var/www/html/hdptar -xvf /root/HDF-3.2.0.0-centos7-rpm.tar.gz.tar -C /var/www/html/hdf Confirm that you can browse to the newly created local repository. 1234567891011#Ambari BaseURLhttp://192.168.110.210/ambari/centos7/2.7.1.0-169/#HDP BaseURLhttp://192.168.110.210/hdp/HDP/centos7/3.0.1.0-187/#HDP-UTIL BaseURLhttp://192.168.110.210/hdp/HDP-UTILS/centos7/1.1.0.22/#HDF BaseURLhttp://192.168.110.210/hdf/HDF/centos7/3.2.0.0-520/ Downloading and Editing the Ambari Repository Configuration File to Use the Local Repository. 123wget -nv http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.1.0/ambari.repo -O /etc/yum.repos.d/ambari.repovi /etc/yum.repos.d/ambari.repo Replace the Ambari baseurl to your local repository. 123456789#VERSION_NUMBER=2.7.1.0-169[ambari-2.7.1.0]#json.url = http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.jsonname=ambari Version - ambari-2.7.1.0baseurl=http://192.168.110.210/ambari/centos7/2.7.1.0-169/gpgcheck=1gpgkey=http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.1.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1 Install and set up Ambari server on the Ambari Server host. 1yum install ambari-server Then, start to set up ambari-server. 1ambari-server setup Run SQL script. 12345mysql -uambari -puse ambari;source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql Start Ambari-server. 1ambari-server start Installing Ambari Cluster Browse Ambari WebUI, then use account admin to log in. Launch Install Wizard. Step 0: Name your cluster. Step 1: Edit BaseURL to your local repository. Step 2: Input a list of hosts using hostname and input your ssh private key of the Ambari server host. Step 3: Confirm Hosts. Step 4: Choose Services. Step 5: Assign Masters. Step 6: Assign Slaves and Clients. Step 7-1: Input usernames and passwords for your services. Step 7-2: Create Databases and users for services, then input information for login. 123456789101112131415161718192021222324252627#Rangercreate database ranger;create user &apos;ranger&apos;@&apos;localhost&apos; identified by &apos;ranger&apos;;create user &apos;ranger&apos;@&apos;master01.ambari.com&apos; identified by &apos;ranger&apos;;grant all on ranger.* to &apos;ranger&apos;@&apos;localhost&apos;;grant all on ranger.* to &apos;ranger&apos;@&apos;master01.ambari.com&apos;;#RangerKMScreate database rangerkms;create user &apos;rangerkms&apos;@&apos;localhost&apos; identified by &apos;rangerkms&apos;;create user &apos;rangerkms&apos;@&apos;master01.ambari.com&apos; identified by &apos;rangerkms&apos;;grant all on rangerkms.* to &apos;rangerkms&apos;@&apos;localhost&apos;;grant all on rangerkms.* to &apos;rangerkms&apos;@&apos;master01.ambari.com&apos;;#Ooziecreate database oozie;create user &apos;oozie&apos;@&apos;localhost&apos; identified by &apos;oozie&apos;;create user &apos;oozie&apos;@&apos;master01.ambari.com&apos; identified by &apos;oozie&apos;;grant all on oozie.* to &apos;oozie&apos;@&apos;localhost&apos;;grant all on oozie.* to &apos;oozie&apos;@&apos;master01.ambari.com&apos;;#Hivecreate database hive;create user &apos;hive&apos;@&apos;localhost&apos; identified by &apos;hive&apos;;create user &apos;hive&apos;@&apos;master01.ambari.com&apos; identified by &apos;hive&apos;;grant all on hive.* to &apos;hive&apos;@&apos;localhost&apos;;grant all on hive.* to &apos;hive&apos;@&apos;master01.ambari.com&apos;; Step 7-3: Follow other steps to complete. Step 8: Review Step 9: Install, Start and Test. Summary. Completion Installing HDF on Ambari 2.7.1 + HDP 3.0.1Regarding HDF installation, I will write another post to describe specifically.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Meeting 401 Http Status Code when Visting Oozie UI by a browser in a Kerberos environment]]></title>
    <url>%2Fposts%2F3f9aa226.html</url>
    <content type="text"><![CDATA[Using your browser such as Firefox or Chrome to access your component services in Ambari with Kerberos Authorization may confront the error message above. The Http status code 401 showing that lacking authorization to access specific resources. So solving this error is to assign specific authorization for the resource that you need to access. https://community.hortonworks.com/questions/212654/nifi-processor-connection-to-schema-registry-with.html Firefox Configuring Firefox Open Firefox and type the “about:config” in the address bar. Specify the hostname of your Ambari cluster to the properties(“network.negotiate-auth.delegation-uris” and “network.negotiate-auth.trusted-uris”) Typing Command “kinit” Assigning authorization to specific service which you want to access. 1kinit -kt [the file path of specific keytab file] [the principle of specific service] Other BrowsersAccessing the website to get concrete solutions for specific browsers. https://ping.force.com/Support/PingFederate/Integrations/How-to-configure-supported-browsers-for-Kerberos-NTLM]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Comparison between Hortonworks and Alibaba]]></title>
    <url>%2Fposts%2F720bb0c8.html</url>
    <content type="text"><![CDATA[公司 Hortonworks Alibaba 大数据产品 Ambari (提供Hadoop生态的管理、运维、升级等功能)HDP (提供大部分Hadoop相关开源组件)HDF (提供数据流相关的开源组件) Dataworks (大数据开发平台，提供在线编辑工具)Dataphin (新一代大数据平台，为阿里的数据中台业务中台铺路)QuickBI (提供全面的报表展示服务，为Martech助力) 市场占有率 国际市场的三驾马车之一（Cloudera, MapR），18年底完成与Cloudera公司的合并 国际市场占有率低（有一部分Flink的原因，Flink16年出现） 收费模式 100%开源，提供收费技术支持及培训 具体需要联系售前 产品特点 100%开源，使用不受任何限制 基础架构及上层服务均需要全套阿里产品，同时数据留存在阿里 技术栈 HadoopHDFSYarnSpark2(DataFrame + SQL)NifiKafkaKerberosZooKeeperRangerOozie…. MaxCompute(阿里大数据处理引擎，基于Hadoop早期版本定制)Blink(基于Flink的定制版，做batch及streaming处理) 主要开发语言 Java, Scala, SQL, 部分可视化拖拽（Nifi） 可视化拖拽, SQL 开发成本 高(需要部署开发环境，同时需要IDE开发工具) 低(配置+SQL，所有开发均在阿里在线开发环境完成) 运维成本 高(对Hadoop生态的优化需要自己完成) 低 第三方服务/系统对接 容易实现 需要看阿里是否支持 技术支持 有成熟的社区及非常完善的文档，基本上遇到的问题都能解决同时Ambari还提供智能支持的收费服务，也有收费培训等 具体暂不清楚，应该会有对应的技术工程师提供完备的服务，省去了花时间自己找解决方案 数据安全 Kerberos+Ranger 阿里产品 部署方式 基于虚机/Docker的部署，微软Azure提供深度支持 独立部署（需要有千万级数据的体量）多租户方式 (适于小体量同时价格便宜)]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cannot start Namenode with Safemode Error]]></title>
    <url>%2Fposts%2Fab80d8c6.html</url>
    <content type="text"><![CDATA[It’s so annoying when you see the namenode error above, and always showing the same error after restarting several times. Fortunately, I found an answer online writen by a hortonworks staff. So, we don’t need to worry about this kind of error, and we can use the command below to ignore this error. 1hadoop dfsadmin -safemode leave Using the command below to get the state of the namenode 1hdfs dfsadmin -safemode get Incorrect steps for stopping your Ambari cluster may casue this kind of error.Following steps below can avoid. Clicking the button “Stop All”. Running the commands below. 123ambari-agent stop ambari-server stop Powering off VMs.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cannot start ambari services with 400 status code]]></title>
    <url>%2Fposts%2F7449623f.html</url>
    <content type="text"><![CDATA[Error MessageYou maybe meet this kind of error message showing below when you click the button “Start All”. Error message: java.lang.IllegalArgumentException: Invalid transition for servicecomponenthost, clusterName=ambari, clusterId=2, serviceName=SMARTSENSE, componentName=ACTIVITY_EXPLORER, hostname=ambari.com, currentState=STOPPING, newDesiredState=STARTED SolutionIt’s so annoying I cannot find any great solutions online until I saw this article in the hortonworks community. Step1: Update the state of components in Ambari So, update the table hostcomponentstate. Step2: Restart Ambari agents and server123ambari-agent restartambari-server restart ReasonStopping your ambari cluster in a exceptional steps will cause this kind of message such as exceptional poweroff. So, we ought to follow correct steps blow to avoid this kind of unexpectable error. Clicking the button “Stop All”. Running the commands below. 123ambari-agent stop ambari-server stop Powering off VMs.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kerberos Authentication Explained]]></title>
    <url>%2Fposts%2Fbfaac82a.html</url>
    <content type="text"><![CDATA[What is KerberosIn Greek mythology, Kerberos is a creature which is a three-headed dog that guards the gate toward the underworld. In a nutshell, it’s a security guardian.Back to our network world, Kerberos is a computer network authorization protocol which allows clients and servers to communicate in a secure manner (symmetric key cryptography). It is designed by MIT. TerminologyKerberos is a protocol which uses within client-server model. Before we explore the authorization process, we ought to know some terms in Kerberos. KDC (Key Distribution Center) Which issues the TGT to the client and contains TGS and AS. AS (Authorization Server) Which forwards the username to a KDC. TGS (Ticket-granting Service/Server) A service which can generate TGT. TGT (Ticket-granting Ticket) A data segment which is time stamped and encryts it by using TGS secret key and returns to the client. principal A service node which guards by Kerberos. SS (Service Server) Also called as principal which provides specific service for clients. SPN (Service Principal Name) The literal meaning of this word. Realm Which encompasses all services that you can access. PrincipalWhen requiring access to a service or host, there are four components that you need to keep back in your mind. AS TGS SS Client So, let’s begin to cross this mysterious jungle. Process 1: Getting TGT The client sends the plaintext to AS. The plaintext contains: name/ID timestamp AS verifies timestamp, then lookups client by client’s username to ensure that the client is a legal principal. If the time gap between the client and AS is upper than 5min, the client will be refused by AS. AS generates a TGT and a TGS session key which used for communicating with TGS. AS sends two message (TGT and TGS session key) back to the client. The client secret key which is created by the client within registration process (Using command “addprinc”). TGT which encrypted by TGS secret key (client cannot decrypt TGT) contains: your name/ID the TGS name/ID timestamp your network address lifetime of TGT TGT session key Info package which encrypted by client secret key which KDC owning it contains: TGS name/ID timestamp lifetime TGS session key The client fetches TGT and TGS session key which can be obtained by decrypting info package. Process 2: Requesting access to a specific service(principal)In this step, you just communicate with TGS. The client sends the plaintext request, the authenticator and TGT to TGS. The plaintext contains: service name/ID lifetime of the ticket for the service The authenticator which encrypted by TGS session key contains: your name/ID timestamp TGS verifies if the service exists. TGS decrypts TGT and the authenticator, then comparing properties. TGS can use TGS secret key to decrypt TGT. So TGS can obtain the TGT session key which can decrypts the authenticator. After that TGS needs to compare the information which is TGT provided by AS to the information which is the authenticator created by the client. TGS sends the service session key and the service ticket. The service ticket which encrypted by service secret key contains: your name/ID service name/ID your network address timestamp lifetime of this service ticket service session key Info package which encrypted by TGS session key contains: service name/ID timestamp lifetime of the service ticket the service session key The client fetches the service ticket and the service session key which can be obtained by decrypting info package. Process 3: Communicating with the serviceFrom now, you just communicate with the service. The client sends the authenticator and the service ticket to the service server.The authenticator which encrypted by the service session key contains: your name/ID timestamp The service decrypts the service ticket and the authenticator, then compares properties.The service server can use the service secret key to decrypt the service ticket. So the service server can obtain the service session key which can decrypts the authenticator. After that the service server needs to compare the information which is the service ticket provided by TGS to the information which is the authenticator created by the client. The service sends the authenticator to the client in order to confirm its identity.The authenticator which encrypted by the service session key contains: the service name/ID timestamp The client decrypts the authenticator, and konws it has been authenticated to use the service by cached the service ticket.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using Flume to transfer data from Kafka to HDFS in Ambari(2.4.2) with Kerberos]]></title>
    <url>%2Fposts%2Fdc7e5002.html</url>
    <content type="text"><![CDATA[Firstly, you don’t need to read following post if your Flume upper than 1.5.2. You can complete your configuration file by following official documents.** Versions of components Ambari: 2.4.2 HDP: 2.4.3 HDFS: 2.7.1.2.4 Kafka: 0.9.0.2.4 Flume: 1.5.2.2.4 Unpredictable problemAssume that using Kafka to receive data from a topic, then store data to HDFS. It is obviously a good choice using kafka source, memory channel and hdfs sink. Nevertheless, it’s not easy like that after searching User Guide(1.5.2). After reading contents above, it is pitiful that Kafka source not supporting kerberos environment because of no properties for specifying principle and keytab file path. So why Flume 1.5.2 not supporting Kafka source? OK. Let’s check out the library within Flume (/user/hdp/2.4.3.0-227/flume/lib). There are some libs specifying Kafka(0.8.2). Well, it’s so weird that Ambari-2.4.2 using Kafka-0.9.0, but Flume-1.5.2 in Ambari-2.4.2 not using Kafka-0.9.0 instead of using Kafka-0.8.2. I don’t know why, but I also found similary problem after googling. Kafka starts to support Kerberos authorization after version 0.9. So it’s obviously that we can’t use Kafka source in Flume in Ambari-2.4.2 with Kerberos. From this moment, we got stuck by this annoying version conflict. After complaining, we also need to solve this problem. But, how?Although Kafka-0.8.2 not supporting Kerberos, it can ought to create a custom Kafka source. Appending Kerberos authorization code within your custom Kafka source that is a reasonable solution for this situation. Reasonable solutionCreating custom Kafka(0.9.0) sourceWriting your custom Kafka source which inherting class AbstractSourceis just like a Kafka consumer. Attention: The dependencies of Kafka need to use the version of 0.9.0. Java codes paste below. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.johnny.flume.source.kafka;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.EventDeliveryException;import org.apache.flume.PollableSource;import org.apache.flume.conf.Configurable;import org.apache.flume.event.EventBuilder;import org.apache.flume.source.AbstractSource;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.nio.charset.Charset;import java.util.*;public class CustomKafkaSource extends AbstractSource implements Configurable, PollableSource &#123; private KafkaConsumer&lt;byte[], String&gt; consumer; @Override public Status process() throws EventDeliveryException &#123; List&lt;Event&gt; eventList = new ArrayList&lt;Event&gt;(); try &#123; ConsumerRecords&lt;byte[], String&gt; records = consumer.poll(1000); Event event; Map&lt;String, String&gt; header; for (ConsumerRecord&lt;byte[], String&gt; record : records) &#123; header = new HashMap&lt;String, String&gt;(); header.put("timestamp", String.valueOf(System.currentTimeMillis())); event = EventBuilder.withBody(record.value(), Charset.forName("UTF-8"), header); eventList.add(event); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return Status.BACKOFF; &#125; getChannelProcessor().processEventBatch(eventList); return Status.READY; &#125; @Override public void configure(Context context) &#123; Properties properties = new Properties(); properties.put("zookeeper.connect", context.getString("zk")); properties.put("group.id", context.getString("groupId")); properties.put("auto.offset.reset", "earliest"); properties.put("bootstrap.servers", context.getString("bootstrapServers")); properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT"); properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true"); this.consumer = new KafkaConsumer&lt;byte[], String&gt;(properties); this.consumer.subscribe(Arrays.asList(context.getString("topics").split(","))); &#125; @Override public synchronized void stop() &#123; super.stop(); &#125;&#125; Using the code “context.getString()” can get specific property value from your configuration file. Generating jar packageUtilizing your build automation tool - such as maven, gradle and etc - to generate your custom Kafka source jar. Replacing Kafka lib and Adding custom Kafka source jarIn this step, you outhgt to download a Kafka jar package (version is 0.9.0), then copy the file “kafka-clients-0.9.0.0.jar” and the file that custom Kafka source jar to your lib path of Flume(/usr/hdp/2.4.3.0-227/flume/lib). After that, you also need to delete old version of Kafka client jar(kafka-clients-0.8.2.0.jar). Other preparation for final point Creating Kafka topic and ensuring that it’s working. 12345678910111213kinit -kt /etc/security/keytabs/kafka.service.keytab kafka/bigdata.com@CONNEXT.COMcd /usr/hdp/2.4.3.0-227/kafka/bin/kafka-topics.sh --create --zookeeper bigdata.com:2181 --replication-factor 1 --partitions 1 --topic flume-kafkabin/kafka-topics.sh --list --zookeeper bigdata.com:2181bin/kafka-topics.sh --describe --zookeeper bigdata.com:2181 --topic flume-kafkabin/kafka-console-producer.sh --topic flume-kafka --broker-list bigdata.com:6667 --security-protocol SASL_PLAINTEXTbin/kafka-console-consumer.sh --topic flume-kafka --zookeeper bigdata.com:2181 --from-beginning --security-protocol SASL_PLAINTEXT Creating your storage folder in your HDFS for saving data. Editing your flume configuration fileFinally, it’s time to reach the peak. Just follow the configuration code below. 1234567891011121314151617181920212223242526272829amk.sources = kamk.sinks = hamk.channels = m#### Sources ####amk.sources.k.type = com.johnny.flume.source.kafka.CustomKafkaSourceamk.sources.k.zk = bigdata.com:2181amk.sources.k.groupId = flume-kafkaamk.sources.k.bootstrapServers = bigdata.com:6667amk.sources.k.topics = flume-kafka#### Sinks ####amk.sinks.h.type = hdfsamk.sinks.h.hdfs.fileType = DataStreamamk.sinks.h.hdfs.path = /johnny/flume/events/%y-%m-%damk.sinks.h.hdfs.filePrefix = eventsamk.sinks.h.hdfs.fileSuffix = .logamk.sinks.h.hdfs.round = trueamk.sinks.h.hdfs.roundValue = 10amk.sinks.h.hdfs.roundUnit = minuteamk.sinks.h.hdfs.useLocalTimeStamp = trueamk.sinks.h.hdfs.kerberosPrincipal = hdfs-bigdata@CONNEXT.COMamk.sinks.h.hdfs.kerberosKeytab = /etc/security/keytabs/hdfs.headless.keytab#### Channels ####amk.channels.m.type = memoryamk.sources.k.channels = mamk.sinks.h.channel = m Others need to noticeSimilarly, we also outght to write a custom Kafka sink in Flume(1.5.2) if we need.The java code pastes below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.johnny.flume.sink.kafka;import org.apache.flume.*;import org.apache.flume.conf.Configurable;import org.apache.flume.sink.AbstractSink;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;public class CustomKafkaSink extends AbstractSink implements Configurable &#123; public KafkaProducer&lt;String, String&gt; producer; public String topic; @Override public Status process() throws EventDeliveryException &#123; Status status = null; Channel ch = getChannel(); Transaction txn = ch.getTransaction(); txn.begin(); try &#123; Event event = ch.take(); if (event == null) &#123; status = Status.BACKOFF; &#125; byte[] byte_message = event.getBody(); //生产者 ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(this.topic, new String(byte_message)); producer.send(record); txn.commit(); status = Status.READY; &#125; catch (Throwable t) &#123; txn.rollback(); status = Status.BACKOFF; if (t instanceof Error) &#123; throw (Error) t; &#125; &#125; finally &#123; txn.close(); &#125; return status; &#125; @Override public void configure(Context context) &#123; Properties properties = new Properties(); properties.put(&quot;bootstrap.servers&quot;, context.getString(&quot;bootstrapServers&quot;)); properties.put(&quot;metadata.broker.list&quot;, context.getString(&quot;brokerList&quot;)); properties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); properties.put(&quot;request.required.acks&quot;, context.getString(&quot;acks&quot;)); properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;); this.producer = new KafkaProducer&lt;String, String&gt;(properties); this.topic = context.getString(&quot;topic&quot;); &#125;&#125; The related configuration file pastes below. 123456#### Sinks ####amk.sinks.k.type = com.johnny.flume.sink.kafka.CustomKafkaSinkamk.sinks.k.bootstrapServers = bigdata.com:6667amk.sinks.k.brokerList = bigdata.com:6667amk.sinks.k.acks = 1amk.sinks.k.topic = flume-kafka]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Kafka</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using Flume in Ambari with Kerberos]]></title>
    <url>%2Fposts%2Fa833198.html</url>
    <content type="text"><![CDATA[What is flume Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. An Event is a unit of data, and events that carrying payloads flows from source to channel to sink. All above running in a flume agent which runs in a JVM. Three key components of Flume: Source The purpose of a source is to receive data from an external client and store it in a configured channel. Channel The channel is just like a bridge which receive data from a source and buffer them till they are consumed by sinks. Sink Sinks can consume data from a channel and deliver it to another destination. The destination of sink might be another agent or a storage. Attention:In HDP3.0, Ambari doesn’t use Flume continuously instead of using Nifi. Kerberos authorizationBefore we use flume, we need to configure it in ambari with kerberos. Visit Ambari UI and click “Flume” service to change configuration file. More importantly, you need to append security content in the end of the property “flume-env template” when you using kafka components with kerberos. Every kafka client needs a JAAS file to get a TGT from TGS. 1export JAVA_OPTS=&quot;$JAVA_OPTS -Djava.security.auth.login.config=/home/flume/kafka-jaas.conf&quot; ConfigurationThere is a simple example that tranferring log data from one pc to another pc, and I paste configuration code below.Using avro-source and avro-sink is a perfect choice to tranfer data from one agent to another agent. One PC configurationsource: exec-sourcechannel: memorysink: avro-sink 12345678910111213141516exec-memory-avro.sources = exec-sourceexec-memory-avro.sinks = avro-sinkexec-memory-avro.channels = memory-channel exec-memory-avro.sources.exec-source.type = execexec-memory-avro.sources.exec-source.command = tail -F /Users/JohnnyLiu/Documents/local_flume/data.logexec-memory-avro.sources.exec-source.shell = /bin/sh -c exec-memory-avro.sinks.avro-sink.type = avroexec-memory-avro.sinks.avro-sink.hostname = bigdata.comexec-memory-avro.sinks.avro-sink.port = 44444 exec-memory-avro.channels.memory-channel.type = memory exec-memory-avro.sources.exec-source.channels = memory-channelexec-memory-avro.sinks.avro-sink.channel = memory-channel Another PC configurationsource: avro-sourcechannel: memorysink: logger 1234567891011121314avro-memory-logger.sources = avro-sourceavro-memory-logger.sinks = logger-sinkavro-memory-logger.channels = memory-channelavro-memory-logger.sources.avro-source.type = avroavro-memory-logger.sources.avro-source.bind = bigdata.comavro-memory-logger.sources.avro-source.port = 44444avro-memory-logger.sinks.logger-sink.type = loggeravro-memory-logger.channels.memory-channel.type = memoryavro-memory-logger.sources.avro-source.channels = memory-channelavro-memory-logger.sinks.logger-sink.channel = memory-channel]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Kerberos</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Running Kafka consumer and producer in Kerberos Authorization]]></title>
    <url>%2Fposts%2F2d304b2.html</url>
    <content type="text"><![CDATA[No! No! No! Not this guy, we are talking about Apache Kafka. What is kafka Kafka is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies. Feature: Scalability Fault-tolerant Multi-source Real-time streaming data Fast Versions of my ambari components Ambari 2.7.1.0 HDP 3.0.1.0 HDF 3.2.0.0 PreparationUpdate configuration propertiesWhen you using Ambari to manage your kafka, you don’t need to complete some annoying installation and configuration steps. But you still need to change or add some properties for kerberos authorization. Change property “listener” in “Kafka Broker” tab. Because of using kerberos, the protocol of the listeners must be updated. 1listeners = SASL_PLAINTEXT://localhost:6667 Update the protocol of brokers’ security. Change your brokers’ protocol (“security.inter.broker.protocol“) in “Advanced kafka-broker” tab. Add “KAFKA_OPTS” for JAAS configuration file. Aftering enabling Kerberos, Ambari sets up a JAAS (Java Authorization and Authorization Service) login configuration file for the Kafka client. Settings in this file will be used for any client (consumer, producer) that connects to a Kerberos-enabled Kafka cluster. You don’t need to change the content of JAAS configuration file, you just need to add a command in “kafka-env template” in “Advanced kafka-env” tab. 12export KAFKA_PLAIN_PARAMS=&quot;-Djava.security.auth.login.config=/usr/hdp/3.0.1.0-187/kafka/conf/kafka_jaas.conf&quot;export KAFKA_OPTS=&quot;$KAFKA_PLAIN_PARAMS $KAFKA_OPTS&quot; You definitely should change the location of your JAAS file. Create a kafka topicIt is a key step that creating a kafaka topic before running a consuemr and a producer.Excepting topic creation command, the other beneficial topic commands show below. Creating 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --create --zookeeper ambari.com:2181 --replication-factor 1 --partitions 1 --topic flume-kafka Listing all topics 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --list --zookeeper ambari.com:2181 Describing specific topic 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --describe --zookeeper ambari.com:2181 --topic flume-kafka Running a Consumer and a Producer Init kerberos authorization for kafka client. 1kinit -kt /etc/security/keytabs/kafka.service.keytab kafka/hostname@REALM.COM Running a consumer 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-consumer.sh --topic flume-kafka --zookeeper ambari.com:2181 --from-beginning --security-protocol SASL_PLAINTEXT The security protocol must be specific. Running a producer 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-producer.sh --topic flume-kafka --broker-list ambari.com:6667 --security-protocol SASL_PLAINTEXT The security protocol must be specific. So, now you can use terminal tool to test your consumer and producer.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using spark-submit to submit a spark application in kerberos environment]]></title>
    <url>%2Fposts%2Fb0e85a85.html</url>
    <content type="text"><![CDATA[Spark-submit is a shell script that allows you to deploy a spark application for execution, kill or request status of spark applications. So, it is generally a depolyment tool for spark application. How spark runs an application Create SparkContext from driver. Request resources from Cluster Manager. CM find Worker Node, then response to spark driver. Spark driver connects to Worker Nodes directly, then communicate each other (such as sending tasks). So, we can notice that serveral components are ciratical - spark driver, CM, Worker Node - in this submission process. And spark can use different CM - pseudo cluster manager, standalone CM, yarn, mesos and kubernets - to request resources and schedule tasks for spark applications. Spark also provides 2 different types of deployment mode - client and cluster - for running spark driver which can creates a SparkContext. Cluster Manager typesCM which acquires resources on the cluster is a critical key componet for a distributed system. Pseudo cluster manager In this kind of non-distributed single-JVM deployment mode, Spark creates all the execution components - driver, excutor and master - in the same single JVM. Using this kind of cluster manager to test your spark application is very convient and easy. Run spark locally with two worker threads: 1--master local[2] Run spark locally with as many as logical cores on your machine: 1--master local[*] Standalone It is a simple cluster manager providing by Spark. We can use it as a development tool and a testing tool among the project. 1--master spark://hostname:port Hadoop Yarn The resources manager in Hadoop 2. I’s a excellent choice in ambari environment. 1--master yarn Apache Mesos A general cluster manager that can run Hadoop MapReduce and service application. Kubernetes (K8S) An open-source system for automating deployment, scaling, and management of containerized applications. Deployment mode types Client Spark driver runs where the job is submitted outside of the cluster. 1--deploy-mode client Cluster By contrast with clident deployment mode, spark driver runs inside the cluster. 1--deploy-mode cluster Steps for using spark-submit command to launch a spark application in ambari with kerberos authorization Get TGTBefore we using this tool, we need to get TGT for spark service. Using command kinit to get ticket. 1kinit -kt /etc/security/keytabs/spark.headless.keytab spark-bigdata@CONNEXT.COM Submit application Pseudo cluster manager Notices: The asterisk can be replaced by specific number that showing how many worker nodes to be applied. And the asterisk is just announced that acquiring worker nodes as many as logical cores on your machine. Specifying package path and your application class name to CM for informing entry point of your spark application. 1234 /usr/hdp/current/spark-client/bin/spark-submit \--master local[*] \--class com.johnny.demo.SparkDemo \/root/spark-demo-1.0.0.jar \ 2. Yarn-cluster **Notices:** * Specifying entry point of your spark application which just likes the mode of pseudo cluster manager. * Your application jar files and other files all need to upload to your cluster. Not local file system. 1234567 /usr/hdp/current/spark-client/bin/spark-submit \--class com.johnny.demo.SparkDemo \--master yarn \--deploy-mode cluster \--files /usr/hdp/current/spark-client/conf/hive-site.xml \--jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar \hdfs://bigdata.com:8020/johnny/oozie/workflow/shell-action/demo1/spark-demo-1.0.0.jar]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Two Ways of Connecting Hive For Developing or Testing]]></title>
    <url>%2Fposts%2F87dabd80.html</url>
    <content type="text"><![CDATA[Hive CLIHive CLI a legacy tool which had two main use cases. The first is that it served as thick client for SQL on Hadoop. And the second is that it served as a command line tool for Hiver Server. Hive Server had beed deprecated and removed from Hive. So, it is not a ideal way to connect Hive by using Hive CLI. Command below for using Hive CLI. 1hive Beeline CLIIt is a client tool for connecting Hive Server2 through JDBC. Then you can use SQL to obtain or process data.Because of using JDBC, there are two tunnels for transmiting data. The first way is zookeeper which is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. The second way is apache thrift server which is RPC framwork for scalable corss-language service development. Before we use beeline, we need to get TGT from TGS for using hive service when your hadoop clusters using kerberos authorization. After that, you can use hive directly. 1kinit -kt [your hive service keytab file path] [your hive service principal] You can checkout your kerberos.csv file to get specific service keytab file path and service principal. Zookeeper 123beeline !connect jdbc:hive2://bigdata.com:2181/sephora;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/bigdata.com@CONNEXT.COM So, you maybe want to get your own jdbc connection url by using zookeeper. You can visit your own Ambari UI, then click “[Services]-&gt;[Hive]”. In your hive dashboard, you can get your own url. After this, we maybe encounter a bug that needs to input username and password. So, just click “Enter” two times because of owning the authorization for hive service. https://issues.apache.org/jira/browse/HIVE-9144 Thrift Server 123beeline!connect jdbc:hive2://bigdata.com:10001/sephora;transportMode=http;httpPath=cliservice;principal=hive/bigdata.com@CONNEXT.COM We can get parameters’ values by visting Ambari UI and checking configuration of Hive. Just like using zookeeper, we also encounter that bug. So, just click “Enter” two times.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Enable Kerberos on Ambari]]></title>
    <url>%2Fposts%2F3f0454be.html</url>
    <content type="text"><![CDATA[Install a new MIT KDCInstall the KDC server Install a new version of the KDC server 1yum install -y krb5-server krb5-libs krb5-workstation Update the KDC server configuration file 1vi /etc/krb5.conf Update the name of realms and ensure the value of “renew_lifetime” is “7d” 1234567[libdefaults]default_realm = BIGDATA.COMdns_lookup_realm = falsedns_lookup_kdc = falseticket_lifetime = 24hrenew_lifetime = 7dforwardable = true Update the item “realms” 12345[realms]BIGDATA.COM = &#123; kdc = master1.bigdata.com admin_server = master1.bigdata.com&#125; Update the item “domain_realm” 123[domain_realm].bigdata.com = BIGDATA.COMbigdata.com = BIGDATA.COM Create the kerberos database1kdb5_util create -s Start the KDC12/etc/rc.d/init.d/krb5kdc start/etc/rc.d/init.d/kadmin start Set up the KDC server to auto-start on boot. 12chkconfig krb5kdc onchkconfig kadmin on Create a Kerberos AdminYou need to create an admin account, then provide this admin credentials for enabling kerberos on ambari. Create a KDC admin 1kadmin.local -q "addprinc admin/admin" Ensure this admin account include the authorization to enter into the specific realms 1vi /var/kerberos/krb5kdc/kadm5.acl 1*/admin@BIGDATA.COM * Restart the kadmin process 1/etc/rc.d/init.d/kadmin restart Install the JCEIf you already use the JDK that contains the JCE, you ought to skip this step. Download the specific version of the JCE file For Oracle JDK 1.8: 1http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html For Oracle JDK 1.7: 1http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html Add the JCE file to the JDK installation direction 1$JAVA_HOME/jre/lib/security/ Enabling the Kerberos Enter into the configuration of the kerberos Choose the type of the KDC Configure the Kerberos Install and test the kerberos client Configure identitiesYou can just click the button “next”. Confirm ConfigurationYou need to download the csv file that contains a list of the principals and keytabs. Stop services Kerberize cluster Start and test services]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Ambari</tag>
        <tag>Kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Recover a Crashed Mysql Database]]></title>
    <url>%2Fposts%2F4cf6882d.html</url>
    <content type="text"><![CDATA[Among the operation of Ambari, we may confront some annoying failure, such as crashed mysql database. So, we can run commands below to recover this kind of problem. Check the status of crashed table. 1check table [table name] Repaire crashed table. 1repaire table [table name] On the other hand, you can use commands below to repair whole crashed tables. 1mysqlcheck -uroot -p --repair --all-databases 1myisamchk --recover *.MYI]]></content>
      <categories>
        <category>Database</category>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Ambari</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tar command in Linux]]></title>
    <url>%2Fposts%2Fd36bfb53.html</url>
    <content type="text"><![CDATA[options:-c: create the archive-x: extract the archive-f: create or extract with specific archive file name-t: displays or lists files in archived file-v: display info-z: using gzip -C: extract files into specific directory not current directory Creating an uncompressed tar archive. 1tar -cvf file.tar directory Creating a compressed tar archive by using gzip. 1tar -cvzf file.tar.gz directory Extracting files from archive. 1tar -xvf file.tar Extracting gzip tar archive. 1tar -xvzf file.tar.gz Extracting files into specific directory. 1tar -xvf file.tar -C /var/html/ Displays or lists files in archived file. 1tar -tvf file.tar]]></content>
      <categories>
        <category>Operation</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql Basic operations for User Account Management]]></title>
    <url>%2Fposts%2Ff4c81e09.html</url>
    <content type="text"><![CDATA[Install mysql server (not mysql package) 1yum -y install mysql-server Start and launch mysql 123service mysqld startmysql -uroot -p Add users. 1CREATE USER 'user_name'@'host' IDENTIFIED BY 'password'; The value of host: %: connect to mysql server from any other machine localhost: just local host ip_address: specific ip address Grant authority to users. 123456789101112 GRANT ALL PRIVILEGES on [db_name].[db_table_name] TO [mysql_user_name]@[the_value_of_host] IDENTIFIED BY '[mysql_user_password]'; ``` [all privileges] can be set with specific authority("select,delete,update,create,drop") [db_name] can be set with all databases("*") [db_table_name] can be set with all tables("*") 3. Update password. ```bash ALTER USER 'user_name'@'host' IDENTIFIED BY 'password'; Delete users. 1DROP USER 'user_name'@'host';]]></content>
      <categories>
        <category>Operation</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Auto-start Services on Boot in Linux]]></title>
    <url>%2Fposts%2F857f09d5.html</url>
    <content type="text"><![CDATA[CentOS6 Show list of services that start on boot. 1chkconfig --list Add a service to auto-start on boot. 12chkconfig --add service_namechkconfig service_name on Confirm script is added successfully 1chkconfig --list service_name Disable an auto-start services. 12chkconfig service_name offchkconfig --del service_name CentOS7 Add a service to auto-start on boot. 1systemctl enable service_name Disable an auto-start service. 1systemctl disable service_name Check the status of auto-start service. 1systemctl status service_name]]></content>
      <categories>
        <category>Operation</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention for Using Markdown code block]]></title>
    <url>%2Fposts%2Fd2fb4425.html</url>
    <content type="text"><![CDATA[When you are using markdown code block, you may meet a strange circumstance that disorder within code blocks. Just like the image blow. So, when you confront same problem like me, you just need to delete your space symbols after your end of code block. If you put one more space after your code block, the system will not recognize that’s a end signal of code block.]]></content>
      <categories>
        <category>Language</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>Hexo</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Step by Step Tutorial for Ambari Installation]]></title>
    <url>%2Fposts%2F7f0f012a.html</url>
    <content type="text"><![CDATA[System RequirementsAt first, we need to think about the setups of VMs. So, I list a table for VMs below. hostname ip operating system RAM disk space package master1.bigdata.com 192.168.110.150 centos6.5 64位 16g 50g ambari-servernamenode slave1.bigdata.com 192.168.110.151 centos6.5 64位 16g 50g ambari-agentdatanode slave2.bigdata.com 192.168.110.152 centos6.5 64位 16g 50g ambari-agentdatanode VM Deployment Login VMware vsphere client system Create 3 VMs with specific system requirements Set the name of VM Choose data storage Choose hardware configuration Load centos6 ios file into CD-ROM when bootingYou can use minimum version of centos. Then you need to click the option “Running system installation with system booting”. Configuration complete Operating system installation Power on the VM Choose the first one. Click “skip” to continue. Choose language, keyborad and timezone. Set root password. Click “Using entire drive” to continue. Choose “Write changes to disk” to continue. Reboot. VM Preparation1. Configuration and Installation for VMs(master and slave servers) Install java. 1yum -y install java Enable network adapter. 123vi /etc/sysconfig/network-scripts/ifcfg-eth0vi /etc/sysconfig/network-scripts/ifcfg-eth1vi /etc/sysconfig/network-scripts/ifcfg-eth2 Then, enable it to run after system booting 1ONBOOT=yes Using tool ping to check status of accessing public network and local network. Change system’s hostname on all servers. Update file(/etc/sysctl.conf), add a item 1kernel.domainname=master1.bigdata.com Update file(/etc/hosts), add doamins. 123192.168.110.150 master1.bigdata.com master1192.168.110.151 slave1.bigdata.com slave1192.168.110.152 slave2.bigdata.com slave2 Update file(/etc/sysconfig/network) 12NETWORKING=yesHOSTNAME=master1.bigdata.com Reboot systems Check the correction of hostname 12hostnamehostname -f Set up password-less SSH on all servers Disable SELinux 1vi /etc/selinux/config Set disabled for SELinux: 1SELINUX=disabled Enable access for using public key. Warning: File name is sshd_config, not ssh_config. 1vi /etc/ssh/sshd_config Delete comment signal(“#”) blow: 123RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys Generate public key and private key on master server. 1ssh-keygen -t rsa Then, click default option for configuration. Create directories and copy public key to slave servers. 123slave$ mkdir ~/.sshslave$ cd ~/.sshslave$ touch authorized_keys Login to master server, then copy public key: 1maseter$ scp ~/.ssh/id_rsa.pub root@slave1:~/.ssh Login to slave server, then copy content from public key to a file(authorized_keys): 1slave$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys **Notice: You need to copy your content of local ssh public key to your local authorized_keys, if you use standalone mode.** Install ntpd on all servers. Check to see if you have installed ntpd. 1rpm -qa | grep ntp If not, install ntpd 1yum -y install ntp Close transparent huge page on all servers. Append code block to file(/etc/rc.local) for closing transparent huge page. 1vi /etc/rc.local code block: 123456if test -f /sys/kernel/mm/transparent_hugepage/enabled; thenecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiif test -f /sys/kernel/mm/transparent_hugepage/defrag; thenecho never &gt; /sys/kernel/mm/transparent_hugepage/defragfi Reboot operating system. Check the status of transparent huge page to ensure the status is “never”. 1cat /sys/kernel/mm/transparent_hugepage/enabled 2. Set up a local repository on master server. Install local repository tool. 1yum install yum-utils createrepo Install http service. 1yum install httpd Start http service. 1service httpd start Install tool wget. 1yum -y install wget Download ambari &amp; hdp package files to master server. 123wget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.2.0/ambari-2.4.2.0-centos6.tar.gzwget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6/HDP-UTILS-1.1.0.20-centos6.tar.gzwget http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.3.0/HDP-2.4.3.0-centos6-rpm.tar.gz Copy ambari and hdp packages to http dir, then untar them. 12345cd /var/www/htmlmkdir hdptar -xvf ambari-2.4.2.0-centos6.tar.gz -C /var/www/htmltar -xvf HDP-2.4.3.0-centos6-rpm.tar.gz -C /var/www/html/hdptar -xvf HDP-UTILS-1.1.0.20-centos6.tar.gz -C /var/www/html/hdp Check repository address for next step. Ambari base URL and gpgkey: 12http://192.168.110.150/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ http://192.168.110.150/AMBARI-2.4.2.0/centos6/2.4.2.0-136/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins HDP base URL and gpgkey: 12http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins HDP-UTILS base URL and gpgkey(gpgkey file same with HDP gpgkey file): 12http://192.168.110.150/hdp/HDP-UTILS-1.1.0.20/repos/centos6http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins Copy repository config files. ambari repo file: 1wget -nv http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.3.0/ambari.repo -O /etc/yum.repos.d/ambari.repo hdp repo file: 1cp /var/www/html/hdp/HDP/centos6/2.x/updates/2.4.3.0/hdp.repo /etc/yum.repos.d/ Update repository config files. You ought to renew base URL and gpgkey URL in your ambari.repo and hdp.repo files. 3. Ambari-server setup on master server Install ambari server 1yum install /var/www/html/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ambari/ambari-server-2.4.2.0-136.x86_64.rpm Run command for ambari setup. 1ambari-server setup If you are not disable the SELinux, you need to press “y”. If your ambari server under user root, you just need to press “n”. If you are not disable iptalbes, you need to press “y” to continue. Select JDK version, then follow steps to install it. If you use a custom jdk, you will maybe meet some troubles that will happen within Ambari Installation. So, choosing option one for recommendation. Choose database type You can choose embeded database. If you choose mysql, you need to install mysql database on master server and input info for ambari setup. Then put a mysql connector into specific path and run a command below to specify the path of connector. 1ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar Run ambari-server 1ambari-server start 4. Ambari-agent setup on slave servers. Copy ambari-agent rpm file to slave servers. 1scp /var/www/html/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ambari/ambari-agent-2.4.2.0-136.x86_64.rpm root@slave_server_ip:~ Install ambari-agent. 1yum install ~/ambari-agent-2.4.2.0-136.x86_64.rpm Renew ambari-server URL in config file. 1vi /etc/ambari-agent/ambari.ini Renew ambari-server URL below: 12[server]hostname = master1.bigdata.com Run ambari-agent. 1ambari-agent start InstallationBefore you launch the ambari installation webUI, you ought to ensure things below. Start the service httpd. Stop the service iptables. Start the ambari-server on master server and ambari-agent on slave servers. If you want to start quickly, it is advisable to choose default options. Open your brower, then access to the WebUI and click the item (“Launch Install Wizard”) URL: 192.168.110.150:8080 Account: admin Password: admin Get Started Select Version Choose local repository, then input base URL of HDP and HDP-UTILS. Install Options Input your master’s hostname and slaves’ hostnames. Then copy your master’s ssh private key into item. Confirm Hosts Just click “ok” to continue. Choose Services Choose what service you want to add into your system. You can add new services into your system later on. Just click “ok” to continue. Assign Masters Assign Slaves and Clients Customize ServicesYou just need to input info for items with red exclamation mark. Review Install, Start and TestIf you get to this step successfully, you can drink a cup of coffee and enjoy this time. SummarySo now, say hello world to Ambari.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using docker command to obtain dockers' IP]]></title>
    <url>%2Fposts%2Fde75b356.html</url>
    <content type="text"><![CDATA[check specific container’s ip address:1docker inspect -f '&#123;&#123;range.NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' your_container_name_or_id check all containers’ ip address:1docker inspect -f '&#123;&#123;.Name&#125;&#125; - &#123;&#123;range.NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' $(docker ps -aq)]]></content>
      <categories>
        <category>Operation</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Some terms in Big Data Ecosystem]]></title>
    <url>%2Fposts%2Faa930aca.html</url>
    <content type="text"><![CDATA[DMP(Data Management Plateform)A palteform for collecting and managing data for digital marketing purpose. DSP(Demand-side Plateform)A palteform that alows buyers of online advertising to manage ad exchange and pay to buy some adversiting banners according to Real-time bidding. SSP(Supply-side Plateform)A palteform that alows web publisher or digital media owner to manage their advertising banners and receive money from this plateform. MDM(Master Data Management)A business method that help companies or enterprises to integrate, manage, store critical data in a single point of reference.These data can provide for other busniess plateforms or business systems(B2B, B2C, CRM, OMS and etc).And master data also are useful for marketing decision and marketing analysis.In china, Alibaba announced a business term(“数据中台”) that is based on MDM.So, in the next few years, we need to build a digital smart plateform to collect all sources of busniess data that can provideinformation for all kinds of business systems or plateform and bread down obstacles between business systems in companies or enterpises.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>DMP</tag>
        <tag>MDM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Hexo]Some tricks for using hexo efficiently]]></title>
    <url>%2Fposts%2Feacbb695.html</url>
    <content type="text"><![CDATA[change theme add theme to your hexo (eg. theme next) 1git clone https://github.com/theme-next/hexo-theme-next themes/next update your blog config file(_config.yml, not your theme config file) 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next add RSS plugin install RSS tools by npm 1npm install --save hexo-generator-feed update your blog config file(_config.yml, not your theme config file) 12345feed: # RSS订阅插件 type: atom path: atom.xml limit: 0plugins: hexo-generate-feed update your theme config file(theme/_config.yml) 1234# Set rss to false to disable feed link.# Leave rss as empty to use site's feed link, and install hexo-generator-feed: `npm install hexo-generator-feed --save`.# Set rss to specific value if you have burned your feed already.rss: /atom.xml change the code block theme1234# Code Highlight theme# Available values: normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: night record the number of viewers12345678910# Show Views/Visitors of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzibusuanzi_count: enable: true total_visitors: true total_visitors_icon: user total_views: true total_views_icon: eye post_views: true post_views_icon: eye generate the excerptThere are two methods for generating excerpt for home page. update theme config file (theme/next/_config.yml)It is not good for you when you are using markdown language because of bad show effect. 12345# Automatically Excerpt. Not recommend.# Use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 300 add markdown code in your post to control excerpt accurately 1&lt;!-- more --&gt; add search plugin install search plugin 1npm install hexo-generator-search --save update blog config file 12345search: path: search.xml field: post format: html limit: 10000 update theme config file 1234567891011# Local search# Dependencies: https://github.com/theme-next/hexo-generator-searchdblocal_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1 # unescape html strings to the readable one unescape: false add share plugin get module from github 12cd themes/nextgit clone https://github.com/theme-next/theme-next-needmoreshare2 source/lib/needsharebutton update theme config file 12345678910111213141516needmoreshare2: enable: true postbottom: enable: true options: iconStyle: box boxForm: horizontal position: bottomCenter networks: Weibo,Wechat,Twitter,Facebook float: enable: false options: iconStyle: box boxForm: horizontal position: middleRight networks: Weibo,Wechat,Twitter,Facebook add google analytics update theme config file 12# Google Analyticsgoogle_analytics: your google track code add reward update theme config file 123456789# Reward# If true, reward would be displayed in every article by default.# And you can show or hide one article specially through add page variable `reward: true/false`.reward: enable: true comment: Thanks For Your Donation! wechatpay: /images/wechatpay.jpg alipay: /images/alipay.jpg #bitcoin: /images/bitcoin.jpg generate a permanent post address install plugin 1npm install hexo-abbrlink --save update blog config file 123456# permalink: :year/:month/:day/:title/# permalink_defaults:permalink: posts/:abbrlink.htmlabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex add comment module (valine) sign up for a leancloud account create a application for your comment system copy AppID and AppKey from your dashboard paste AppID and AppKey into your theme config file 123456789101112valine: enable: true # When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version. appid: axxDLm5SBUPG3DcHu79Jwn00-gzGzoHsz # your leancloud application appid appkey: XKifYmgTXg3JM7XThQYo5NiL # your leancloud application appkey notify: false # mail notifier, See: https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: Just go go # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail # custom comment header pageSize: 10 # pagination size visitor: true # leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html comment_count: true # if false, comment count will only be displayed in post page, not in home page create classes in your leancloud account to store your data [not required] you can change your valine lanuage (themes/next/layout/_third-party/comments/valine.swig) 12345678910111213new Valine(&#123; el: '#comments', lang: 'en', verify: &#123;&#123; theme.valine.verify &#125;&#125;, notify: &#123;&#123; theme.valine.notify &#125;&#125;, appId: '&#123;&#123; theme.valine.appid &#125;&#125;', appKey: '&#123;&#123; theme.valine.appkey &#125;&#125;', placeholder: '&#123;&#123; theme.valine.placeholder &#125;&#125;', avatar: '&#123;&#123; theme.valine.avatar &#125;&#125;', meta: guest, pageSize: '&#123;&#123; theme.valine.pageSize &#125;&#125;' || 10, visitor: &#123;&#123; theme.valine.visitor &#125;&#125; &#125;); [not required] you can also add white list for security use image plugin install “hexo-asset-image” plugin 1npm install hexo-asset-image --save update your blog config file (_config.yml) 1post_asset_folder: true if you use abbrlink plugin to change your permalink attribute in your blog config file, you need to edit this plugin to fit your config (node_modules/hexo-asset-image/index.js). Then you add a code block into this javascript file. 123var link = data.permalink; link = link.replace('.html', '/'); //新增加，针对修改后的permalinkvar beginPos = getPosition(link, '/', 3) + 1; Ok, you can generate your new static html file. 12hexo cleanhexo g add creative commons module to your blogIt is convenient and useful for bloger to announce your blog copyright announcement. So, you can add this below your posts or add this in your profile information page. update your theme config file 1234567# Creative Commons 4.0 International License.# https://creativecommons.org/share-your-work/licensing-types-examples# Available values: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zerocreative_commons: license: by-nc-sa sidebar: true post: true]]></content>
      <categories>
        <category>Front-end</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Hexo]Create a blog with hexo in github]]></title>
    <url>%2Fposts%2Ffe740d97.html</url>
    <content type="text"><![CDATA[copy local ssh public key to github install npm, git install hexo by npm 1npm install init hexo 1hexo init install other softwares by npm 1npm install edit file “_config.yml” and write your git address 1234deploy: type: git repository: github项目地址 branch: master install hexo deployment tool 1npm install hexo-deployer-git --save generate static pages by hexo 1hexo g deploy static pages to github 1hexo d using local tool to pre-check your blog 1hexo s access to your github blog 1http://用户名.github.io]]></content>
      <categories>
        <category>Front-end</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
</search>
