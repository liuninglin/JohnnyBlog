<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kerberos Authentication Explained]]></title>
    <url>%2Fposts%2Fbfaac82a.html</url>
    <content type="text"><![CDATA[What is KerberosIn Greek mythology, Kerberos is a creature which is a three-headed dog that guards the gate toward the underworld. In a nutshell, it’s a security guardian.Back to our network world, Kerberos is a computer network authorization protocol which allows clients and servers to communicate in a secure manner (symmetric key cryptography). It is designed by MIT. TerminologyKerberos is a protocol which uses within client-server model. Before we explore the authorization process, we ought to know some terms in Kerberos. KDC (Key Distribution Center) Which issues the TGT to the client and contains TGS and AS. AS (Authorization Server) Which forwards the username to a KDC. TGS (Ticket-granting Service/Server) A service which can generate TGT. TGT (Ticket-granting Ticket) A data segment which is time stamped and encryts it by using TGS secret key and returns to the client. principal A service node which guards by Kerberos. SS (Service Server) Also called as principal which provides specific service for clients. SPN (Service Principal Name) The literal meaning of this word. Realm Which encompasses all services that you can access. PrincipalWhen requiring access to a service or host, there are four components that you need to keep back in your mind. AS TGS SS Client So, let’s begin to cross this mysterious jungle. Process 1: Getting TGT The client sends the plaintext to AS. The plaintext contains: name/ID timestamp AS verifies timestamp, then lookups client by client’s username to ensure that the client is a legal principal. If the time gap between the client and AS is upper than 5min, the client will be refused by AS. AS generates a TGT and a TGS session key which used for communicating with TGS. AS sends two message (TGT and TGS session key) back to the client. The client secret key which is created by the client within registration process (Using command “addprinc”). TGT which encrypted by TGS secret key (client cannot decrypt TGT) contains: your name/ID the TGS name/ID timestamp your network address lifetime of TGT TGT session key Info package which encrypted by client secret key which KDC owning it contains: TGS name/ID timestamp lifetime TGS session key The client fetches TGT and TGS session key which can be obtained by decrypting info package. Process 2: Requesting access to a specific service(principal)In this step, you just communicate with TGS. The client sends the plaintext request, the authenticator and TGT to TGS. The plaintext contains: service name/ID lifetime of the ticket for the service The authenticator which encrypted by TGS session key contains: your name/ID timestamp TGS verifies if the service exists. TGS decrypts TGT and the authenticator, then comparing properties. TGS can use TGS secret key to decrypt TGT. So TGS can obtain the TGT session key which can decrypts the authenticator. After that TGS needs to compare the information which is TGT provided by AS to the information which is the authenticator created by the client. TGS sends the service session key and the service ticket. The service ticket which encrypted by service secret key contains: your name/ID service name/ID your network address timestamp lifetime of this service ticket service session key Info package which encrypted by TGS session key contains: service name/ID timestamp lifetime of the service ticket the service session key The client fetches the service ticket and the service session key which can be obtained by decrypting info package. Process 3: Communicating with the serviceFrom now, you just communicate with the service. The client sends the authenticator and the service ticket to the service server.The authenticator which encrypted by the service session key contains: your name/ID timestamp The service decrypts the service ticket and the authenticator, then compares properties.The service server can use the service secret key to decrypt the service ticket. So the service server can obtain the service session key which can decrypts the authenticator. After that the service server needs to compare the information which is the service ticket provided by TGS to the information which is the authenticator created by the client. The service sends the authenticator to the client in order to confirm its identity.The authenticator which encrypted by the service session key contains: the service name/ID timestamp The client decrypts the authenticator, and konws it has been authenticated to use the service by cached the service ticket.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Using Flume to transfer data from Kafka to HDFS in Ambari(2.4.2) with Kerberos]]></title>
    <url>%2Fposts%2Fdc7e5002.html</url>
    <content type="text"><![CDATA[Firstly, you don’t need to read following post if your Flume upper than 1.5.2. You can complete your configuration file by following official documents. Versions of components Ambari: 2.4.2 HDP: 2.4.3 HDFS: 2.7.1.2.4 Kafka: 0.9.0.2.4 Flume: 1.5.2.2.4 Unpredictable problemAssume that using Kafka to receive data from a topic, then store data to HDFS. It is obviously a good choice using kafka source, memory channel and hdfs sink. Nevertheless, it’s not easy like that after searching User Guide(1.5.2). After reading contents above, it is pitiful that Kafka source not supporting kerberos environment because of no properties for specifying principle and keytab file path. So why Flume 1.5.2 not supporting Kafka source? OK. Let’s check out the library within Flume (/user/hdp/2.4.3.0-227/flume/lib). There are some libs specifying Kafka(0.8.2). Well, it’s so weird that Ambari-2.4.2 using Kafka-0.9.0, but Flume-1.5.2 in Ambari-2.4.2 not using Kafka-0.9.0 instead of using Kafka-0.8.2. I don’t know why, but I also found similary problem after googling. Kafka starts to support Kerberos authorization after version 0.9. So it’s obviously that we can’t use Kafka source in Flume in Ambari-2.4.2 with Kerberos. From this moment, we got stuck by this annoying version conflict. After complaining, we also need to solve this problem. But, how?Although Kafka-0.8.2 not supporting Kerberos, it can ought to create a custom Kafka source. Appending Kerberos authorization code within your custom Kafka source that is a reasonable solution for this situation. Reasonable solutionCreating custom Kafka(0.9.0) sourceWriting your custom Kafka source which inherting class AbstractSourceis just like a Kafka consumer. Attention: The dependencies of Kafka need to use the version of 0.9.0. Java codes paste below. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.johnny.flume.source.kafka;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.EventDeliveryException;import org.apache.flume.PollableSource;import org.apache.flume.conf.Configurable;import org.apache.flume.event.EventBuilder;import org.apache.flume.source.AbstractSource;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.nio.charset.Charset;import java.util.*;public class CustomKafkaSource extends AbstractSource implements Configurable, PollableSource &#123; private KafkaConsumer&lt;byte[], String&gt; consumer; @Override public Status process() throws EventDeliveryException &#123; List&lt;Event&gt; eventList = new ArrayList&lt;Event&gt;(); try &#123; ConsumerRecords&lt;byte[], String&gt; records = consumer.poll(1000); Event event; Map&lt;String, String&gt; header; for (ConsumerRecord&lt;byte[], String&gt; record : records) &#123; header = new HashMap&lt;String, String&gt;(); header.put("timestamp", String.valueOf(System.currentTimeMillis())); event = EventBuilder.withBody(record.value(), Charset.forName("UTF-8"), header); eventList.add(event); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return Status.BACKOFF; &#125; getChannelProcessor().processEventBatch(eventList); return Status.READY; &#125; @Override public void configure(Context context) &#123; Properties properties = new Properties(); properties.put("zookeeper.connect", context.getString("zk")); properties.put("group.id", context.getString("groupId")); properties.put("auto.offset.reset", "earliest"); properties.put("bootstrap.servers", context.getString("bootstrapServers")); properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT"); properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true"); this.consumer = new KafkaConsumer&lt;byte[], String&gt;(properties); this.consumer.subscribe(Arrays.asList(context.getString("topics").split(","))); &#125; @Override public synchronized void stop() &#123; super.stop(); &#125;&#125; Using the code “context.getString()” can get specific property value from your configuration file. Generating jar packageUtilizing your build automation tool - such as maven, gradle and etc - to generate your custom Kafka source jar. Replacing Kafka lib and Adding custom Kafka source jarIn this step, you outhgt to download a Kafka jar package (version is 0.9.0), then copy the file “kafka-clients-0.9.0.0.jar” and the file that custom Kafka source jar to your lib path of Flume(/usr/hdp/2.4.3.0-227/flume/lib). After that, you also need to delete old version of Kafka client jar(kafka-clients-0.8.2.0.jar). Other preparation for final point Creating Kafka topic and ensuring that it’s working. 12345678910111213kinit -kt /etc/security/keytabs/kafka.service.keytab kafka/bigdata.com@CONNEXT.COMcd /usr/hdp/2.4.3.0-227/kafka/bin/kafka-topics.sh --create --zookeeper bigdata.com:2181 --replication-factor 1 --partitions 1 --topic flume-kafkabin/kafka-topics.sh --list --zookeeper bigdata.com:2181bin/kafka-topics.sh --describe --zookeeper bigdata.com:2181 --topic flume-kafkabin/kafka-console-producer.sh --topic flume-kafka --broker-list bigdata.com:6667 --security-protocol SASL_PLAINTEXTbin/kafka-console-consumer.sh --topic flume-kafka --zookeeper bigdata.com:2181 --from-beginning --security-protocol SASL_PLAINTEXT Creating your storage folder in your HDFS for saving data. Editing your flume configuration fileFinally, it’s time to reach the peak. Just follow the configuration code below. 1234567891011121314151617181920212223242526272829amk.sources = kamk.sinks = hamk.channels = m#### Sources ####amk.sources.k.type = com.johnny.flume.source.kafka.CustomKafkaSourceamk.sources.k.zk = bigdata.com:2181amk.sources.k.groupId = flume-kafkaamk.sources.k.bootstrapServers = bigdata.com:6667amk.sources.k.topics = flume-kafka#### Sinks ####amk.sinks.h.type = hdfsamk.sinks.h.hdfs.fileType = DataStreamamk.sinks.h.hdfs.path = /johnny/flume/events/%y-%m-%damk.sinks.h.hdfs.filePrefix = eventsamk.sinks.h.hdfs.fileSuffix = .logamk.sinks.h.hdfs.round = trueamk.sinks.h.hdfs.roundValue = 10amk.sinks.h.hdfs.roundUnit = minuteamk.sinks.h.hdfs.useLocalTimeStamp = trueamk.sinks.h.hdfs.kerberosPrincipal = hdfs-bigdata@CONNEXT.COMamk.sinks.h.hdfs.kerberosKeytab = /etc/security/keytabs/hdfs.headless.keytab#### Channels ####amk.channels.m.type = memoryamk.sources.k.channels = mamk.sinks.h.channel = m Others need to noticeSimilarly, we also outght to write a custom Kafka sink in Flume(1.5.2) if we need.The java code pastes below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.johnny.flume.sink.kafka;import org.apache.flume.*;import org.apache.flume.conf.Configurable;import org.apache.flume.sink.AbstractSink;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;public class CustomKafkaSink extends AbstractSink implements Configurable &#123; public KafkaProducer&lt;String, String&gt; producer; public String topic; @Override public Status process() throws EventDeliveryException &#123; Status status = null; Channel ch = getChannel(); Transaction txn = ch.getTransaction(); txn.begin(); try &#123; Event event = ch.take(); if (event == null) &#123; status = Status.BACKOFF; &#125; byte[] byte_message = event.getBody(); //生产者 ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(this.topic, new String(byte_message)); producer.send(record); txn.commit(); status = Status.READY; &#125; catch (Throwable t) &#123; txn.rollback(); status = Status.BACKOFF; if (t instanceof Error) &#123; throw (Error) t; &#125; &#125; finally &#123; txn.close(); &#125; return status; &#125; @Override public void configure(Context context) &#123; Properties properties = new Properties(); properties.put(&quot;bootstrap.servers&quot;, context.getString(&quot;bootstrapServers&quot;)); properties.put(&quot;metadata.broker.list&quot;, context.getString(&quot;brokerList&quot;)); properties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); properties.put(&quot;request.required.acks&quot;, context.getString(&quot;acks&quot;)); properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;); this.producer = new KafkaProducer&lt;String, String&gt;(properties); this.topic = context.getString(&quot;topic&quot;); &#125;&#125; The related configuration file pastes below. 123456#### Sinks ####amk.sinks.k.type = com.johnny.flume.sink.kafka.CustomKafkaSinkamk.sinks.k.bootstrapServers = bigdata.com:6667amk.sinks.k.brokerList = bigdata.com:6667amk.sinks.k.acks = 1amk.sinks.k.topic = flume-kafka]]></content>
  </entry>
  <entry>
    <title><![CDATA[Using Flume in Ambari with Kerberos]]></title>
    <url>%2Fposts%2Fa833198.html</url>
    <content type="text"><![CDATA[What is flume Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. An Event is a unit of data, and events that carrying payloads flows from source to channel to sink. All above running in a flume agent which runs in a JVM. Three key components of Flume: Source The purpose of a source is to receive data from an external client and store it in a configured channel. Channel The channel is just like a bridge which receive data from a source and buffer them till they are consumed by sinks. Sink Sinks can consume data from a channel and deliver it to another destination. The destination of sink might be another agent or a storage. Attention:In HDP3.0, Ambari doesn’t use Flume continuously instead of using Nifi. Kerberos authorizationBefore we use flume, we need to configure it in ambari with kerberos. Visit Ambari UI and click “Flume” service to change configuration file. More importantly, you need to append security content in the end of the property “flume-env template” when you using kafka components with kerberos. Every kafka client needs a JAAS file to get a TGT from TGS. 1export JAVA_OPTS=&quot;$JAVA_OPTS -Djava.security.auth.login.config=/home/flume/kafka-jaas.conf&quot; ConfigurationThere is a simple example that tranferring log data from one pc to another pc, and I paste configuration code below.Using avro-source and avro-sink is a perfect choice to tranfer data from one agent to another agent. One PC configurationsource: exec-sourcechannel: memorysink: avro-sink 12345678910111213141516exec-memory-avro.sources = exec-sourceexec-memory-avro.sinks = avro-sinkexec-memory-avro.channels = memory-channel exec-memory-avro.sources.exec-source.type = execexec-memory-avro.sources.exec-source.command = tail -F /Users/JohnnyLiu/Documents/local_flume/data.logexec-memory-avro.sources.exec-source.shell = /bin/sh -c exec-memory-avro.sinks.avro-sink.type = avroexec-memory-avro.sinks.avro-sink.hostname = bigdata.comexec-memory-avro.sinks.avro-sink.port = 44444 exec-memory-avro.channels.memory-channel.type = memory exec-memory-avro.sources.exec-source.channels = memory-channelexec-memory-avro.sinks.avro-sink.channel = memory-channel Another PC configurationsource: avro-sourcechannel: memorysink: logger `avro-memory-logger.sources = avro-sourceavro-memory-logger.sinks = logger-sinkavro-memory-logger.channels = memory-channel avro-memory-logger.sources.avro-source.type = avroavro-memory-logger.sources.avro-source.bind = bigdata.comavro-memory-logger.sources.avro-source.port = 44444 avro-memory-logger.sinks.logger-sink.type = logger avro-memory-logger.channels.memory-channel.type = memory avro-memory-logger.sources.avro-source.channels = memory-channelavro-memory-logger.sinks.logger-sink.channel = memory-channel]]></content>
  </entry>
  <entry>
    <title><![CDATA[Running Kafka consumer and producer in Kerberos Authorization]]></title>
    <url>%2Fposts%2F2d304b2.html</url>
    <content type="text"><![CDATA[What is kafka Kafka is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies. Feature: Scalability Fault-tolerant Multi-source Real-time streaming data Fast Versions of my ambari components Ambari 2.7.1.0 HDP 3.0.1.0 HDF 3.2.0.0 PreparationUpdate configuration propertiesWhen you using Ambari to manage your kafka, you don’t need to complete some annoying installation and configuration steps. But you still need to change or add some properties for kerberos authorization. Change property “listener” in “Kafka Broker” tab. Because of using kerberos, the protocol of the listeners must be updated. 1listeners = SASL_PLAINTEXT://localhost:6667 Update the protocol of brokers’ security. Change your brokers’ protocol (“security.inter.broker.protocol“) in “Advanced kafka-broker” tab. Add “KAFKA_OPTS” for JAAS configuration file. Aftering enabling Kerberos, Ambari sets up a JAAS (Java Authorization and Authorization Service) login configuration file for the Kafka client. Settings in this file will be used for any client (consumer, producer) that connects to a Kerberos-enabled Kafka cluster. You don’t need to change the content of JAAS configuration file, you just need to add a command in “kafka-env template” in “Advanced kafka-env” tab. 123456789101112 export KAFKA_PLAIN_PARAMS=&quot;-Djava.security.auth.login.config=/usr/hdp/3.0.1.0-187/kafka/conf/kafka_jaas.conf&quot; export KAFKA_OPTS=&quot;$KAFKA_PLAIN_PARAMS $KAFKA_OPTS&quot; ``` You definitely should change the location of your JAAS file.### Create a kafka topicIt is a key step that creating a kafaka topic before running a consuemr and a producer.Excepting topic creation command, the other beneficial topic commands show below.1. Creating /usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh –create –zookeeper ambari.com:2181 –replication-factor 1 –partitions 1 –topic flume-kafka 122. Listing all topics /usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh –list –zookeeper ambari.com:2181 12 3. Describing specific topic /usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh –describe –zookeeper ambari.com:2181 –topic flume-kafka 1234## Running a Consumer and a Producer1. Init kerberos authorization for kafka client. kinit -kt /etc/security/keytabs/kafka.service.keytab kafka/hostname@REALM.COM 12 2. Running a consumer /usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-consumer.sh –topic flume-kafka –zookeeper ambari.com:2181 –from-beginning –security-protocol SASL_PLAINTEXT 123 The security protocol must be specific. 3. Running a producer /usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-producer.sh –topic flume-kafka –broker-list ambari.com:6667 –security-protocol SASL_PLAINTEXT ` The security protocol must be specific. So, now you can use terminal tool to test your consumer and producer.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Using spark-submit to submit a spark application in kerberos environment]]></title>
    <url>%2Fposts%2Fb0e85a85.html</url>
    <content type="text"><![CDATA[Spark-submit is a shell script that allows you to deploy a spark application for execution, kill or request status of spark applications. So, it is generally a depolyment tool for spark application. How spark runs an application Create SparkContext from driver. Request resources from Cluster Manager. CM find Worker Node, then response to spark driver. Spark driver connects to Worker Nodes directly, then communicate each other (such as sending tasks). So, we can notice that serveral components are ciratical - spark driver, CM, Worker Node - in this submission process. And spark can use different CM - pseudo cluster manager, standalone CM, yarn, mesos and kubernets - to request resources and schedule tasks for spark applications. Spark also provides 2 different types of deployment mode - client and cluster - for running spark driver which can creates a SparkContext. Cluster Manager typesCM which acquires resources on the cluster is a critical key componet for a distributed system. Pseudo cluster manager In this kind of non-distributed single-JVM deployment mode, Spark creates all the execution components - driver, excutor and master - in the same single JVM. Using this kind of cluster manager to test your spark application is very convient and easy. Run spark locally with two worker threads: 1--master local[2] Run spark locally with as many as logical cores on your machine: 1--master local[*] Standalone It is a simple cluster manager providing by Spark. We can use it as a development tool and a testing tool among the project. 1--master spark://hostname:port Hadoop Yarn The resources manager in Hadoop 2. I’s a excellent choice in ambari environment. 1--master yarn Apache Mesos A general cluster manager that can run Hadoop MapReduce and service application. Kubernetes (K8S) An open-source system for automating deployment, scaling, and management of containerized applications. Deployment mode types Client Spark driver runs where the job is submitted outside of the cluster. 1--deploy-mode client Cluster By contrast with clident deployment mode, spark driver runs inside the cluster. 1--deploy-mode cluster Steps for using spark-submit command to launch a spark application in ambari with kerberos authorization Get TGTBefore we using this tool, we need to get TGT for spark service. Using command kinit to get ticket. 1kinit -kt /etc/security/keytabs/spark.headless.keytab spark-bigdata@CONNEXT.COM Submit application Pseudo cluster manager Notices: The asterisk can be replaced by specific number that showing how many worker nodes to be applied. And the asterisk is just announced that acquiring worker nodes as many as logical cores on your machine. Specifying package path and your application class name to CM for informing entry point of your spark application. 1234 /usr/hdp/current/spark-client/bin/spark-submit \--master local[*] \--class com.johnny.demo.SparkDemo \/root/spark-demo-1.0.0.jar \ 2. Yarn-cluster **Notices:** * Specifying entry point of your spark application which just likes the mode of pseudo cluster manager. * Your application jar files and other files all need to upload to your cluster. Not local file system. 1234567 /usr/hdp/current/spark-client/bin/spark-submit \--class com.johnny.demo.SparkDemo \--master yarn \--deploy-mode cluster \--files /usr/hdp/current/spark-client/conf/hive-site.xml \--jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar \hdfs://bigdata.com:8020/johnny/oozie/workflow/shell-action/demo1/spark-demo-1.0.0.jar]]></content>
  </entry>
  <entry>
    <title><![CDATA[The Two Ways of Connecting Hive For Developing or Testing]]></title>
    <url>%2Fposts%2F87dabd80.html</url>
    <content type="text"><![CDATA[Hive CLIHive CLI a legacy tool which had two main use cases. The first is that it served as thick client for SQL on Hadoop. And the second is that it served as a command line tool for Hiver Server. Hive Server had beed deprecated and removed from Hive. So, it is not a ideal way to connect Hive by using Hive CLI. Command below for using Hive CLI. 1hive Beeline CLIIt is a client tool for connecting Hive Server2 through JDBC. Then you can use SQL to obtain or process data.Because of using JDBC, there are two tunnels for transmiting data. The first way is zookeeper which is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. The second way is apache thrift server which is RPC framwork for scalable corss-language service development. Before we use beeline, we need to get TGT from TGS for using hive service when your hadoop clusters using kerberos authorization. After that, you can use hive directly. 1kinit -kt [your hive service keytab file path] [your hive service principal] You can checkout your kerberos.csv file to get specific service keytab file path and service principal. Zookeeper 123beeline !connect jdbc:hive2://bigdata.com:2181/sephora;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/bigdata.com@CONNEXT.COM So, you maybe want to get your own jdbc connection url by using zookeeper. You can visit your own Ambari UI, then click “[Services]-&gt;[Hive]”. In your hive dashboard, you can get your own url. After this, we maybe encounter a bug that needs to input username and password. So, just click “Enter” two times because of owning the authorization for hive service. https://issues.apache.org/jira/browse/HIVE-9144 Thrift Server 123beeline!connect jdbc:hive2://bigdata.com:10001/sephora;transportMode=http;httpPath=cliservice;principal=hive/bigdata.com@CONNEXT.COM We can get parameters’ values by visting Ambari UI and checking configuration of Hive. Just like using zookeeper, we also encounter that bug. So, just click “Enter” two times.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux Command: netstat]]></title>
    <url>%2Fposts%2F187005a0.html</url>
    <content type="text"><![CDATA[Definition source: https://www.tecmint.com/20-netstat-commands-for-linux-network-management/ netstat(network statistics) is a command line tool for monitoring connections both incoming and outoging as well as viewing routing tables, interface statistics etc. OptionsSo, let’s begin to learn some useful options for netstat. -a Show all. -n Show networks as numbers not hostnames. -p Show protocol. Most repeated commandYou can use practical command below to find listening programs 1netstat -anp | grep 8080 And you also can use telnet to check specific port. 1telnet [hostname or ip] [port]]]></content>
  </entry>
  <entry>
    <title><![CDATA[Setting Up a Hive Connection with Kerberos using Apache JDBC Drivers]]></title>
    <url>%2Fposts%2F7c77d84.html</url>
    <content type="text"><![CDATA[PreparationBefore we start to code, we need to ensure that we already have a excellent Ambari environment with Kerberos. Then we ought to check the version of services in ambari. Log in the Ambari UI, then click the button on top of the website. Now, we can obtain the versions of all components. Remember the versions of HDFS and Hive. Coding Specifying the version of the dependency of Hive JDBC Ensuring your Hive JDBC URLWe can use]]></content>
  </entry>
  <entry>
    <title><![CDATA[Enable Kerberos on Ambari]]></title>
    <url>%2Fposts%2F3f0454be.html</url>
    <content type="text"><![CDATA[Install a new MIT KDCInstall the KDC server Install a new version of the KDC server 1yum install -y krb5-server krb5-libs krb5-workstation Update the KDC server configuration file 1vi /etc/krb5.conf Update the name of realms and ensure the value of “renew_lifetime” is “7d” 1234567[libdefaults]default_realm = BIGDATA.COMdns_lookup_realm = falsedns_lookup_kdc = falseticket_lifetime = 24hrenew_lifetime = 7dforwardable = true Update the item “realms” 12345[realms]BIGDATA.COM = &#123; kdc = master1.bigdata.com admin_server = master1.bigdata.com&#125; Update the item “domain_realm” 123[domain_realm].bigdata.com = BIGDATA.COMbigdata.com = BIGDATA.COM Create the kerberos database1kdb5_util create -s Start the KDC12/etc/rc.d/init.d/krb5kdc start/etc/rc.d/init.d/kadmin start Set up the KDC server to auto-start on boot. 12chkconfig krb5kdc onchkconfig kadmin on Create a Kerberos AdminYou need to create an admin account, then provide this admin credentials for enabling kerberos on ambari. Create a KDC admin 1kadmin.local -q "addprinc admin/admin" Ensure this admin account include the authorization to enter into the specific realms 1vi /var/kerberos/krb5kdc/kadm5.acl 1*/admin@BIGDATA.COM * Restart the kadmin process 1/etc/rc.d/init.d/kadmin restart Install the JCEIf you already use the JDK that contains the JCE, you ought to skip this step. Download the specific version of the JCE file For Oracle JDK 1.8: 1http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html For Oracle JDK 1.7: 1http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html Add the JCE file to the JDK installation direction 1$JAVA_HOME/jre/lib/security/ Enabling the Kerberos Enter into the configuration of the kerberos Choose the type of the KDC Configure the Kerberos Install and test the kerberos client Configure identities You can just click the button “next”. Confirm Configuration You need to download the csv file that contains a list of the principals and keytabs. Stop services Kerberize cluster Start and test services]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Kerberos</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Recover a Crashed Mysql Database]]></title>
    <url>%2Fposts%2F4cf6882d.html</url>
    <content type="text"><![CDATA[Among the operation of Ambari, we may confront some annoying failure, such as crashed mysql database. So, we can run commands below to recover this kind of problem. Check the status of crashed table. 1check table [table name] Repaire crashed table. 1repaire table [table name] On the other hand, you can use commands below to repair whole crashed tables. 1mysqlcheck -uroot -p --repair --all-databases 1myisamchk --recover *.MYI]]></content>
      <categories>
        <category>Database</category>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Ambari</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tar command in Linux]]></title>
    <url>%2Fposts%2Fd36bfb53.html</url>
    <content type="text"><![CDATA[options:-c: create the archive-x: extract the archive-f: create or extract with specific archive file name-t: displays or lists files in archived file-v: display info-z: using gzip -C: extract files into specific directory not current directory Creating an uncompressed tar archive. 1tar -cvf file.tar directory Creating a compressed tar archive by using gzip. 1tar -cvzf file.tar.gz directory Extracting files from archive. 1tar -xvf file.tar Extracting gzip tar archive. 1tar -xvzf file.tar.gz Extracting files into specific directory. 1tar -xvf file.tar -C /var/html/ Displays or lists files in archived file. 1tar -tvf file.tar]]></content>
      <categories>
        <category>Operation</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql Basic operations for User Account Management]]></title>
    <url>%2Fposts%2Ff4c81e09.html</url>
    <content type="text"><![CDATA[Install mysql server (not mysql package) 1yum -y install mysql-server Start and launch mysql 123service mysqld startmysql -uroot -p Add users. 1CREATE USER 'user_name'@'host' IDENTIFIED BY 'password'; The value of host: %: connect to mysql server from any other machine localhost: just local host ip_address: specific ip address Grant authority to users. 123456789101112 GRANT ALL PRIVILEGES on [db_name].[db_table_name] TO [mysql_user_name]@[the_value_of_host] IDENTIFIED BY '[mysql_user_password]'; ``` [all privileges] can be set with specific authority("select,delete,update,create,drop") [db_name] can be set with all databases("*") [db_table_name] can be set with all tables("*") 3. Update password. ```bash ALTER USER 'user_name'@'host' IDENTIFIED BY 'password'; Delete users. 1DROP USER 'user_name'@'host';]]></content>
      <categories>
        <category>Operation</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Auto-start Services on Boot in Linux]]></title>
    <url>%2Fposts%2F857f09d5.html</url>
    <content type="text"><![CDATA[CentOS6 Show list of services that start on boot. 1chkconfig --list Add a service to auto-start on boot. 12chkconfig --add service_namechkconfig service_name on Confirm script is added successfully 1chkconfig --list service_name Disable an auto-start services. 12chkconfig service_name offchkconfig --del service_name CentOS7 Add a service to auto-start on boot. 1systemctl enable service_name Disable an auto-start service. 1systemctl disable service_name Check the status of auto-start service. 1systemctl status service_name]]></content>
      <categories>
        <category>Operation</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention for Using Markdown code block]]></title>
    <url>%2Fposts%2Fd2fb4425.html</url>
    <content type="text"><![CDATA[When you are using markdown code block, you may meet a strange circumstance that disorder within code blocks. Just like the image blow. So, when you confront same problem like me, you just need to delete your space symbols after your end of code block. If you put one more space after your code block, the system will not recognize that’s a end signal of code block.]]></content>
      <categories>
        <category>Language</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>Hexo</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Step by Step Tutorial for Ambari Installation]]></title>
    <url>%2Fposts%2F7f0f012a.html</url>
    <content type="text"><![CDATA[System RequirementsAt first, we need to think about the setups of VMs. So, I list a table for VMs below. hostname ip operating system RAM disk space package master1.bigdata.com 192.168.110.150 centos6.5 64位 16g 50g ambari-servernamenode slave1.bigdata.com 192.168.110.151 centos6.5 64位 16g 50g ambari-agentdatanode slave2.bigdata.com 192.168.110.152 centos6.5 64位 16g 50g ambari-agentdatanode VM Deployment Login VMware vsphere client system Create 3 VMs with specific system requirements Set the name of VM Choose data storage Choose hardware configuration Load centos6 ios file into CD-ROM when booting You can use minimum version of centos. Then you need to click the option “Running system installation with system booting”. Configuration complete Operating system installation Power on the VM Choose the first one. Click “skip” to continue. Choose language, keyborad and timezone. Set root password. Click “Using entire drive” to continue. Choose “Write changes to disk” to continue. Reboot. VM Preparation1. Configuration and Installation for VMs(master and slave servers) Install java. 1yum -y install java Enable network adapter. 123vi /etc/sysconfig/network-scripts/ifcfg-eth0vi /etc/sysconfig/network-scripts/ifcfg-eth1vi /etc/sysconfig/network-scripts/ifcfg-eth2 Then, enable it to run after system booting 1ONBOOT=yes Using tool ping to check status of accessing public network and local network. Change system’s hostname on all servers. Update file(/etc/sysctl.conf), add a item 1kernel.domainname=master1.bigdata.com Update file(/etc/hosts), add doamins. 123192.168.110.150 master1.bigdata.com master1192.168.110.151 slave1.bigdata.com slave1192.168.110.152 slave2.bigdata.com slave2 Update file(/etc/sysconfig/network) 12NETWORKING=yesHOSTNAME=master1.bigdata.com Reboot systems Check the correction of hostname 12hostnamehostname -f Set up password-less SSH on all servers Disable SELinux 1vi /etc/selinux/config Set disabled for SELinux: 1SELINUX=disabled Enable access for using public key. Warning: File name is sshd_config, not ssh_config. 1vi /etc/ssh/sshd_config Delete comment signal(“#”) blow: 123RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys Generate public key and private key on master server. 1ssh-keygen -t rsa Then, click default option for configuration. Create directories and copy public key to slave servers. 123slave$ mkdir ~/.sshslave$ cd ~/.sshslave$ touch authorized_keys Login to master server, then copy public key: 1maseter$ scp ~/.ssh/id_rsa.pub root@slave1:~/.ssh Login to slave server, then copy content from public key to a file(authorized_keys): 1slave$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys **Notice: You need to copy your content of local ssh public key to your local authorized_keys, if you use standalone mode.** Install ntpd on all servers. Check to see if you have installed ntpd. 1rpm -qa | grep ntp If not, install ntpd 1yum -y install ntp Close transparent huge page on all servers. Append code block to file(/etc/rc.local) for closing transparent huge page. 1vi /etc/rc.local code block: 123456if test -f /sys/kernel/mm/transparent_hugepage/enabled; thenecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiif test -f /sys/kernel/mm/transparent_hugepage/defrag; thenecho never &gt; /sys/kernel/mm/transparent_hugepage/defragfi Reboot operating system. Check the status of transparent huge page to ensure the status is “never”. 1cat /sys/kernel/mm/transparent_hugepage/enabled 2. Set up a local repository on master server. Install local repository tool. 1yum install yum-utils createrepo Install http service. 1yum install httpd Start http service. 1service httpd start Install tool wget. 1yum -y install wget Download ambari &amp; hdp package files to master server. 123wget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.2.0/ambari-2.4.2.0-centos6.tar.gzwget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6/HDP-UTILS-1.1.0.20-centos6.tar.gzwget http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.3.0/HDP-2.4.3.0-centos6-rpm.tar.gz Copy ambari and hdp packages to http dir, then untar them. 12345cd /var/www/htmlmkdir hdptar -xvf ambari-2.4.2.0-centos6.tar.gz -C /var/www/htmltar -xvf HDP-2.4.3.0-centos6-rpm.tar.gz -C /var/www/html/hdptar -xvf HDP-UTILS-1.1.0.20-centos6.tar.gz -C /var/www/html/hdp Check repository address for next step. Ambari base URL and gpgkey: 12http://192.168.110.150/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ http://192.168.110.150/AMBARI-2.4.2.0/centos6/2.4.2.0-136/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins HDP base URL and gpgkey: 12http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins HDP-UTILS base URL and gpgkey(gpgkey file same with HDP gpgkey file): 12http://192.168.110.150/hdp/HDP-UTILS-1.1.0.20/repos/centos6http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins Copy repository config files. ambari repo file: 1wget -nv http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.3.0/ambari.repo -O /etc/yum.repos.d/ambari.repo hdp repo file: 1cp /var/www/html/hdp/HDP/centos6/2.x/updates/2.4.3.0/hdp.repo /etc/yum.repos.d/ Update repository config files. You ought to renew base URL and gpgkey URL in your ambari.repo and hdp.repo files. 3. Ambari-server setup on master server Install ambari server 1yum install /var/www/html/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ambari/ambari-server-2.4.2.0-136.x86_64.rpm Run command for ambari setup. 1ambari-server setup If you are not disable the SELinux, you need to press “y”. If your ambari server under user root, you just need to press “n”. If you are not disable iptalbes, you need to press “y” to continue. Select JDK version, then follow steps to install it. If you use a custom jdk, you will maybe meet some troubles that will happen within Ambari Installation. So, choosing option one for recommendation. Choose database type You can choose embeded database. If you choose mysql, you need to install mysql database on master server and input info for ambari setup. Then put a mysql connector into specific path and run a command below to specify the path of connector. 1ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar Run ambari-server 1ambari-server start 4. Ambari-agent setup on slave servers. Copy ambari-agent rpm file to slave servers. 1scp /var/www/html/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ambari/ambari-agent-2.4.2.0-136.x86_64.rpm root@slave_server_ip:~ Install ambari-agent. 1yum install ~/ambari-agent-2.4.2.0-136.x86_64.rpm Renew ambari-server URL in config file. 1vi /etc/ambari-agent/ambari.ini Renew ambari-server URL below: 12[server]hostname = master1.bigdata.com Run ambari-agent. 1ambari-agent start InstallationBefore you launch the ambari installation webUI, you ought to ensure things below. Start the service httpd. Stop the service iptables. Start the ambari-server on master server and ambari-agent on slave servers. If you want to start quickly, it is advisable to choose default options. Open your brower, then access to the WebUI and click the item (“Launch Install Wizard”) URL: 192.168.110.150:8080 Account: admin Password: admin Get Started Select Version Choose local repository, then input base URL of HDP and HDP-UTILS. Install Options Input your master’s hostname and slaves’ hostnames. Then copy your master’s ssh private key into item. Confirm Hosts Just click “ok” to continue. Choose Services Choose what service you want to add into your system. You can add new services into your system later on. Just click “ok” to continue. Assign Masters Assign Slaves and Clients Customize ServicesYou just need to input info for items with red exclamation mark. Review Install, Start and TestIf you get to this step successfully, you can drink a cup of coffee and enjoy this time. SummarySo now, say hello world to Ambari.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Ambari</tag>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using docker command to obtain dockers' IP]]></title>
    <url>%2Fposts%2Fde75b356.html</url>
    <content type="text"><![CDATA[check specific container’s ip address:1docker inspect -f '&#123;&#123;range.NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' your_container_name_or_id check all containers’ ip address:1docker inspect -f '&#123;&#123;.Name&#125;&#125; - &#123;&#123;range.NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' $(docker ps -aq)]]></content>
      <categories>
        <category>Operation</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Some terms in Big Data Ecosystem]]></title>
    <url>%2Fposts%2Faa930aca.html</url>
    <content type="text"><![CDATA[DMP(Data Management Plateform)A palteform for collecting and managing data for digital marketing purpose. DSP(Demand-side Plateform)A palteform that alows buyers of online advertising to manage ad exchange and pay to buy some adversiting banners according to Real-time bidding. SSP(Supply-side Plateform)A palteform that alows web publisher or digital media owner to manage their advertising banners and receive money from this plateform. MDM(Master Data Management)A business method that help companies or enterprises to integrate, manage, store critical data in a single point of reference.These data can provide for other busniess plateforms or business systems(B2B, B2C, CRM, OMS and etc).And master data also are useful for marketing decision and marketing analysis.In china, Alibaba announced a business term(“数据中台”) that is based on MDM.So, in the next few years, we need to build a digital smart plateform to collect all sources of busniess data that can provideinformation for all kinds of business systems or plateform and bread down obstacles between business systems in companies or enterpises.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>DMP</tag>
        <tag>MDM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Hexo]Some tricks for using hexo efficiently]]></title>
    <url>%2Fposts%2Feacbb695.html</url>
    <content type="text"><![CDATA[change theme add theme to your hexo (eg. theme next) 1git clone https://github.com/theme-next/hexo-theme-next themes/next update your blog config file(_config.yml, not your theme config file) 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next add RSS plugin install RSS tools by npm 1npm install --save hexo-generator-feed update your blog config file(_config.yml, not your theme config file) 12345feed: # RSS订阅插件 type: atom path: atom.xml limit: 0plugins: hexo-generate-feed update your theme config file(theme/_config.yml) 1234# Set rss to false to disable feed link.# Leave rss as empty to use site's feed link, and install hexo-generator-feed: `npm install hexo-generator-feed --save`.# Set rss to specific value if you have burned your feed already.rss: /atom.xml change the code block theme1234# Code Highlight theme# Available values: normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: night record the number of viewers12345678910# Show Views/Visitors of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzibusuanzi_count: enable: true total_visitors: true total_visitors_icon: user total_views: true total_views_icon: eye post_views: true post_views_icon: eye generate the excerptThere are two methods for generating excerpt for home page. update theme config file (theme/next/_config.yml)It is not good for you when you are using markdown language because of bad show effect. 12345# Automatically Excerpt. Not recommend.# Use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 300 add markdown code in your post to control excerpt accurately 1&lt;!-- more --&gt; add search plugin install search plugin 1npm install hexo-generator-search --save update blog config file 12345search: path: search.xml field: post format: html limit: 10000 update theme config file 1234567891011# Local search# Dependencies: https://github.com/theme-next/hexo-generator-searchdblocal_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1 # unescape html strings to the readable one unescape: false add share plugin get module from github 12cd themes/nextgit clone https://github.com/theme-next/theme-next-needmoreshare2 source/lib/needsharebutton update theme config file 12345678910111213141516needmoreshare2: enable: true postbottom: enable: true options: iconStyle: box boxForm: horizontal position: bottomCenter networks: Weibo,Wechat,Twitter,Facebook float: enable: false options: iconStyle: box boxForm: horizontal position: middleRight networks: Weibo,Wechat,Twitter,Facebook add google analytics update theme config file 12# Google Analyticsgoogle_analytics: your google track code add reward update theme config file 123456789# Reward# If true, reward would be displayed in every article by default.# And you can show or hide one article specially through add page variable `reward: true/false`.reward: enable: true comment: Thanks For Your Donation! wechatpay: /images/wechatpay.jpg alipay: /images/alipay.jpg #bitcoin: /images/bitcoin.jpg generate a permanent post address install plugin 1npm install hexo-abbrlink --save update blog config file 123456# permalink: :year/:month/:day/:title/# permalink_defaults:permalink: posts/:abbrlink.htmlabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex add comment module (valine) sign up for a leancloud account create a application for your comment system copy AppID and AppKey from your dashboard paste AppID and AppKey into your theme config file 123456789101112valine: enable: true # When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version. appid: axxDLm5SBUPG3DcHu79Jwn00-gzGzoHsz # your leancloud application appid appkey: XKifYmgTXg3JM7XThQYo5NiL # your leancloud application appkey notify: false # mail notifier, See: https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: Just go go # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail # custom comment header pageSize: 10 # pagination size visitor: true # leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html comment_count: true # if false, comment count will only be displayed in post page, not in home page create classes in your leancloud account to store your data [not required] you can change your valine lanuage (themes/next/layout/_third-party/comments/valine.swig) 12345678910111213new Valine(&#123; el: '#comments', lang: 'en', verify: &#123;&#123; theme.valine.verify &#125;&#125;, notify: &#123;&#123; theme.valine.notify &#125;&#125;, appId: '&#123;&#123; theme.valine.appid &#125;&#125;', appKey: '&#123;&#123; theme.valine.appkey &#125;&#125;', placeholder: '&#123;&#123; theme.valine.placeholder &#125;&#125;', avatar: '&#123;&#123; theme.valine.avatar &#125;&#125;', meta: guest, pageSize: '&#123;&#123; theme.valine.pageSize &#125;&#125;' || 10, visitor: &#123;&#123; theme.valine.visitor &#125;&#125; &#125;); [not required] you can also add white list for security use image plugin install “hexo-asset-image” plugin 1npm install hexo-asset-image --save update your blog config file (_config.yml) 1post_asset_folder: true if you use abbrlink plugin to change your permalink attribute in your blog config file, you need to edit this plugin to fit your config (node_modules/hexo-asset-image/index.js). Then you add a code block into this javascript file. 123var link = data.permalink; link = link.replace('.html', '/'); //新增加，针对修改后的permalinkvar beginPos = getPosition(link, '/', 3) + 1; Ok, you can generate your new static html file. 12hexo cleanhexo g add creative commons module to your blogIt is convenient and useful for bloger to announce your blog copyright announcement. So, you can add this below your posts or add this in your profile information page. update your theme config file # Creative Commons 4.0 International License. # https://creativecommons.org/share-your-work/licensing-types-examples # Available values: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zero creative_commons: license: by-nc-sa sidebar: true post: true]]></content>
      <categories>
        <category>Front-end</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Hexo]Create a blog with hexo in github]]></title>
    <url>%2Fposts%2Ffe740d97.html</url>
    <content type="text"><![CDATA[copy local ssh public key to github install npm, git install hexo by npm 1npm install init hexo 1hexo init install other softwares by npm 1npm install edit file “_config.yml” and write your git address 1234deploy: type: git repository: github项目地址 branch: master install hexo deployment tool 1npm install hexo-deployer-git --save generate static pages by hexo 1hexo g deploy static pages to github 1hexo d using local tool to pre-check your blog 1hexo s access to your github blog 1http://用户名.github.io]]></content>
      <categories>
        <category>Front-end</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
</search>
