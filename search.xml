<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Several Pivotal Conceptions about Hive]]></title>
    <url>%2Fposts%2F449d41fb.html</url>
    <content type="text"><![CDATA[What is Apache Hive?As the data grew in size, there was also the scarcity of Java developers who can write complex MapReduce jobs for Hadoop. Hence the advent of Hive comes which is created on top of Hadoop itself. Hive provides a SQL like a language termed HiveQL interface for users to extract data from a Hadoop system. With the simplicity provided by Hive to transform simple SQL queries into Hadoop’s MapReduce jobs, and runs them against a Hadoop cluster. Apache Hive is well suited for Data warehousing applications in which case the data is structured, static and also formatted. As there are certain design constraints on Hive, it does not provide row-wise updates and inserts (which is coined as the biggest disadvantage of using Hive). As most Hive queries turn out into Map to Reduce jobs these queries will have higher latency due to start up overhead. Based on these details, Hive is NOT a Relational database Design for OLTP (stands for Online Transaction Processing) Language for real-time queries and row-level updates Features of HiveWith the basic understanding of what Apache Hive is, let us now take a look at all the features that are provided with this component of the Hadoop ecosystem: Hive stores the schema details in a database and processes the data into HDFS Hive is designed for OLAP (stands for Online Analytics Processing) Hive provides SQL like language for querying data, named as HiveQL or HQL (do not misinterpret this with HQL from Hibernate, which stands for Hibernate Query Language). Hive is a very fast, scalable and extensible component within the Hadoop ecosystem. External TableIf there is data that is already existing in the HDFS cluster of Hadoop then an external Hive table is created to describe the data. These tables are called External tables, because they are going to be residing in the path specified by the LOCATION properties instead of the default warehouse directory (as described in the above paragraph). When the data is stored in the external tables and when it is dropped, the metadata table is deleted but then the data is kept as is. This means that Hive evidently ignores the data that is presently residing in the path specified by LOCATION property and is left untouched forever. If you want to delete such data, then use the command to achieve the same: 1hadoop fs –rmr ‘tablename’ For example, we can add EXTERNAL keyword for specifying the external table. 12345create external table table_name ( id int, dtDontQuery string, name string) Internal or Managed TableThe tables that are created with the Hadoop Hive’s context, is very much similar to tables that are created on any of the RDBMS systems. Each of the tables that get created is associated with a directory configured within the ${HIVE_HOME}/conf/hive-site.xml in the Hadoop HDFS cluster. By default, on a Linux machine, it is this path /user/hive/warehouse in HDFS. If there is a /user/hive/warehouse/match created by Hive in HDFS for a match table. All the data for the table is recorded in the same folder as mentioned above and hence such tables are called INTERNAL or MANAGED tables. When the data resides in the internal tables, then Hive takes the full responsibility of maintaining the life-cycle of the data and the table in itself. Hence it is evident that the data is removed the moment when the internal tables are dropped. By default Hive creates managed tables such as SQL below. 12345create table table_name ( id int, dtDontQuery string, name string) Partitioning of TableHive stores tables in partitions. Partitions are used to divide the table into related parts. Partitions make data querying more efficient. For example, using SQL below. 123456create table table_name ( id int, dtDontQuery string, name string)partitioned by (date string) Bucketing of TableAs we all know, Partition helps in increasing efficiency when performing a query on a table. Instead of scanning the whole table, it will only scan for the partitioned set and does not scan or operate on the unpartitioned sets, which helps us to provide results in lesser time and the details will be displayed very quickly because of Hive Partition. Now, let’s assume a condition that there is a huge dataset. At times, even after partitioning on a particular field or fields, the partitioned file size doesn’t match with the actual expectation and remains huge and we want to manage the partition results into different parts. To overcome this problem of partitioning, Hive provides Bucketing concept, which allows the user to divide table data sets into more manageable parts. The Bucketing concept is based on Hash function, which depends on the type of the bucketing column. Records which are bucketed by the same column will always be saved in the same bucket. In Hive Partition, each partition will be created as directory. But in Hive Buckets, each bucket will be created as file. Bucketing can also be done even without partitioning on Hive tables. With the help of CLUSTERED BY clause and optional SORTED BY clause in CREATE TABLE statement we can create bucketed tables. We can create a bucketed_user table with above-given requirement with the help of the below HiveQL. 12345678910111213141516CREATE TABLE bucketed_user(firstname VARCHAR(64), lastname VARCHAR(64), address STRING, city VARCHAR(64),state VARCHAR(64), post STRING, phone1 VARCHAR(64), phone2 STRING, email STRING, web STRING )COMMENT ‘A bucketed sorted user table’ PARTITIONED BY (country VARCHAR(64))CLUSTERED BY (state) SORTED BY (city) INTO 32 BUCKETS STORED AS SEQUENCEFILE; Row Format (Delimited / SerDe)SerDeBasically, SerDe is a short name for Serializer/Deserializer. Deserializer The Hive deserializer converts record (string or binary) into a Java object that Hive can process. HDFS files –&gt; InputFileFormat –&gt; &lt;key, value&gt; –&gt; Deserializer –&gt; Row object Serializer Now, the Hive serializer will take this Java object, convert it into suitable format that can be stored into HDFS. Row object –&gt; Serializer –&gt; &lt;key, value&gt; –&gt; OutputFileFormat –&gt; HDFS files So, basically, a SerDe is responsible for converting the record bytes into something that can be used by Hive. Hive comes with several SerDe like JSon SerDe for JSon files, CSV SerDe for CSV files, etc. For example, using JsonSerDe: 12ROW FORMAT SERDE'org.apache.hive.hcatalog.data.JsonSerDe' For more specific SerDes, we can check the official site: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RowFormats&amp;SerDe Row FormatYou can create tables with a custom SerDe or using a native SerDe. A native SerDe is used if ROW FORMAT is not specified or ROW FORMAT DELIMITED is specified.Use the SERDE clause to create a table with a custom SerDe. Here is an example. 123456789CREATE TABLE table_name(id INT, name STRING, published_year INT)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ‘,’STORED AS TEXTFILE ROW FORMAT DELIMITED: This line is telling Hive to expect the file to contain one row per line. So basically, we are telling Hive that when it finds a newline character that means is a new record. Fields TerminatedFor example: 123456789CREATE TABLE table_name(id INT, name STRING, published_year INT)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ‘,’STORED AS TEXTFILE FIELDS TERMINATED BY ‘,’: This is really similar to the one above, but instead of meaning rows this one means columns, this way Hive knows what delimiter you are using in your files to separate each column. If none is set the default will be used which is ctrl-A. Storage FormatHive supports built-in and custom-developed file formats. STORED AS TEXTFILE Stored as plain text files. TEXTFILE is the default file format, unless the configuration parameter hive.default.fileformat has a different setting. STORED AS SEQUENCEFILE Stored as compressed Sequence File. STORED AS AVRO Stored as Avro format. STORED AS JSONFILE Stored as Json file format. STORED AS ORC Stored as Optimized Row Columnar (ORC) file format. For example: 123456789CREATE TABLE table_name(id INT, name STRING, published_year INT)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ‘,’STORED AS TEXTFILE STORED AS TEXTFILE: This is to tell Hive what type of file to expect. Import CSV File Step 1 : If we want to import data from external CSV file which using comma to separate columns, we can use the CREATE TABLE statement SQL below. 123456789101112create table table_name( id string, name string, age string)row format serde'org.apache.hadoop.hive.serde2.OpenCSVSerde'withSERDEPROPERTIES("separatorChar"=",","quotechar"="\"")STORED AS TEXTFILE Step 2 : Loading data from CSV file of HDFS. 1load data local inpath '/home/hive/table_name.csv' overwrite into table table_name;]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction of NiFi]]></title>
    <url>%2Fposts%2F60c88280.html</url>
    <content type="text"><![CDATA[Brief of NiFiApache NiFi is a software project from the Apache Software Foundation designed to automate the flow of data between software systems. Software development and the commercial support is currently offered by Hortonworks (now merged into Cloudera), who acquired NiFi’s originator, Onyara Inc. Apache NiFi supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic. Some of the high-level capabilities and objectives of Apache NiFi include: Web-based user interface Seamless experience between design, control, feedback, and monitoring Highly configurable Loss tolerant vs guaranteed delivery Low latency vs high throughput Dynamic prioritization Flow can be modified at runtime Back pressure Data Provenance Track dataflow from beginning to end Designed for extension Build your own processors and more Enables rapid development and effective testing Secure SSL, SSH, HTTPS, encrypted content, etc… Multi-tenant authorization and internal authorization/policy management For more information about the usage of NiFi: https://nifi.apache.org/docs/nifi-docs/html/getting-started.html Components of NiFi Processor Providing abundant processors for processing distinct data types, connecting processors or other data processing. Different purposes of the processor: Data Ingestion Processor such as GetXXX Data Transformation Processor Data Egress/Sending Processor such as PutXXX Routing and Mediation Processor Databases Access Processor Attribute Extraction Processor System Integration Processor Splitting and Aggregation Processor Http and UDP Processor AWS Processor Processor Group Combining disparate processors into one group for easy management.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>NiFi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive-based Enterprise Data Warehouse(EDW)]]></title>
    <url>%2Fposts%2F3465d938.html</url>
    <content type="text"><![CDATA[Prerequisite Establishing a complete Hadoop ecosystem such as using Ambari. For how to establish a complete Hadoop ecosystem by Ambari, you can check the posts - “Step by Step Tutorial for Ambari Installation” or “Tutorial for Latest Ambari(2.7.1)”. Installing Hive for enterprise data warehouse (EDW). Offering visual Hive query tools such as Hue. Installing NiFi for processing data flow itinerantly. Technical Architecture Business Requirements Sync order data from external MySQL database into Hive daily Compute order amount by orderID daily Compute total order amount daily Sync total order amount to external MySQL database daily Preparations Creating databases and tables for Hive of Hadoop ecosystem and MySQL of the external database separately. Inserting several order data into the external MySQL database for syncing. Design of a series of NiFi flow Firstly, we can create a processor group to package overall processors for easy management. Based on business requirements, I split requirements into main three tasks - syncing data from the external database, computing and analyzing data and syncing data to the external database. The image above shows the main four NiFi processor groups which contain a series of NiFi processor and I will illustrate all four NiFi processor groups separately. NiFi Group 1 : Group_SynOrdersFromMySQL This flow which consists of processor ReplaceText, processor PutHive3QL, processor ExecuteSQL, and processor PutHive3Streaming shows how to grab data from the external MySQL database. Before showing the configuration of each processor, I want to shows a chart to explain the tasks of this flow. Next, I will state the configuration of each processor in each task step by step. Dropping Hive table partition which belongs to that day. This task contains two processors - ReplaceText and PutHive3QL. Processor ReplaceText This processor can replace the identified text segment to an alternative one. But, why we need this processor in task 1? Because the next processor does not provide a SQL input source, so we need a container for loading SQL to the Hive processor. Adding drop SQL to blank “Replacement Value”. Drop SQL: 1ALTER TABLE s_shop_sephora_order_delta_orc DROP IF EXISTS PARTITION(dt='$&#123;now():format('yyyyMMdd')&#125;'); Using built-in code ${now():format(‘yyyyMMdd’)} to get current day code. Processor PutHive3QL PutHive3QL can connect to Hive and execute SQL generated from the previous processor. Clicking the right arrow in the pic above to configure the Hive3ConnectionPool. If you do not have that connection pool, you need to create a new one. The configuration of this connection pool shows blow. Querying order data from the external database. Using processor ExecuteSQL and configuration shows below. And DBConnectionPool shows below. Querying SQL: 1select * from test.sephora_order where 1=1 Saving data to a specific Hive table. Using processor PutHive3Streaming for inserting dataset to a specific Hive table. Configuring AvroReader because of the type of the previous dataset. Until now, we can test the whole flow for accuracy. NiFi Group 2 : Group_ComputePaymentNumberByOrder In this flow, we just use the processor to execute Hive SQL for computing data or analyzing data. Processor ReplaceText The same with the previous processor in the last task, this processor just used for storing SQL. Computing SQL: 12insert overwrite table default.temp_order_result_orc partition(dt='$&#123;now():format('yyyyMMdd')&#125;')select order_num,province,city,region,address,(cast(order_amount AS FLOAT) + 1) from default.s_shop_sephora_order_delta_orc where dt='$&#123;now():format('yyyyMMdd')&#125;'; Processor PutHive3QL NiFi Group 3 : Group_ConvergePaymentNumByOrder In this step, we need to compute the total order amount. Processor ReplaceText The Configuration shows below. Computing SQL: 12insert overwrite table t_order_convergence_orc partition(dt='$&#123;now():format('yyyyMMdd')&#125;')select order_num,sum(total) from temp_order_result_orc where dt='$&#123;now():format('yyyyMMdd')&#125;' group by order_num; Processor PutHive3QL The configuration shows below. NiFi Group 4 : Group_SynTotalPaymentNum2MySQLThis is the last step for this whole POC and the overview flow chart shows below. In this step, I split into five processors for querying result data from the Hive table and saving them to the specific external MySQL table. Next, I will state the function and the configuration of each processor respectively. Processor SelectHive3QL Querying analysis result data from the Hive table and qualifying output format “Avro”. Since we only have two output formats - Avro and CSV, we can convert Avro to SQL successively. Querying Data: 1select order_num_str,total,CURRENT_TIMESTAMP as update_time from default.t_order_convergence_orc where dt = '$&#123;now():format('yyyyMMdd')&#125;' Processor SplitAvro In this processor, we need to split the Avro dataset because the next processors cannot process the dataset. Processor ConvertAvroToJSON Before converting to SQL, we need to convert to JSON firstly. Processor ConvertJSONToSQL Specifying table name of the external MySQL database in this processor. Processor PutSQL SummaryTo sum up, we can use NiFi as a scheduling tool to process data itinerantly since NiFi provides abundant processors for distinct processing. For now, we can run and test this whole processor group.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>NiFi</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installing HDF on Ambari 2.7.1 + HDP 3.0.1]]></title>
    <url>%2Fposts%2F8c49992b.html</url>
    <content type="text"><![CDATA[Installed Ambari and HDP are prerequisites for installing HDF on HDP clusters. So, after that, you can follow the steps below to complete installing HDF. Downloading the installation package. Accessing the URL “https://docs.cloudera.com/HDPDocuments/HDF3/HDF-3.3.1/release-notes/content/hdf_repository_locations.html&quot; to acquire the download location for your specific OS type. Copy the location for “HDF Management Pack” and “HDF Repo” and download these resources on the host where you installed Ambari. Installing the HDF Management Pack. Stop all services on your Ambari. Stop your Ambari by command. 1ambari-server stop Run the command below on the node where the Ambari locates. 123ambari-server install-mpack \--mpack=/tmp/hdf-ambari-mpack-&lt;version&gt;.tar.gz \--verbose Start your Ambari. 1ambari-server start Adding HDF service to your HDP cluster. Unzipping your HDF installation package to your HTTP server and ensure that you can access the HDF installation resources. Signing in your Ambari by user admin and registering HDF-3.2. After that, you will notice a brand new stack component named “HDF-3.2.0.0” appearing on the page “Versions”. Following steps on Ambari Service Wizard to install NiFi. So now, you can install the component NiFi contained in HDF by following steps on Ambari Service Wizard.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tutorial for Latest Ambari(2.7.1)]]></title>
    <url>%2Fposts%2F937f0590.html</url>
    <content type="text"><![CDATA[Before I start to install Ambari 2.7.1, I wrote some key information that I need to use later. Key Information hostname ip operating system RAM disk space cores of CPU package master01.ambari.com 192.168.110.210 CentOS-7-x86_64-Minimal-1611.iso 20g 100g 16 ambari-server slave01.ambari.com 192.168.110.211 CentOS-7-x86_64-Minimal-1611.iso 20g 100g 16 ambari-agent slave02.ambari.com 192.168.110.212 CentOS-7-x86_64-Minimal-1611.iso 20g 100g 16 ambari-agent VM Setting and CentOS7 InstallationVM SettingUpload the IOS fileBefore we start to create VMs and install CentOS7, we ought to upload CentOS7 installation file (using minimal version) to Datacenter in VMware Vsphere. Choose a specific data node in your Datacenter, then upload files. Create VMs Visit and login vsphere Web Client. Create a folder for collecting all nodes. Create a VM node in a specific folder that created in the last step. Input the name of VM Choose computing resources. Choose the storage node. Choose the compatibility of your VM. Choose the version of your VM. Input the number of your CPU cores, the memory size, the hard-drive size and the IOS file path of your CentOS7. CentOS7 Installation Power on the VM. Choose the language, then click the button “Continue”. Start to update setting(changing date&amp;time, specifying installation destination, turning on the network). Specify the installation destination. Turn on the network or update the configuration of the network. Finally, update the password of the user root. File PreparationWe can download the requisite installation file from the official website below. 12345678#Ambarihttps://docs.hortonworks.com/HDPDocuments/Ambari-2.7.1.0/bk_ambari-installation/content/ambari_repositories.html#HDPhttps://docs.hortonworks.com/HDPDocuments/Ambari-2.7.1.0/bk_ambari-installation/content/hdp_30_repositories.html#HDFhttps://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.2.0/release-notes/content/hdf_repository_locations.html Ambari installation file. ambari-2.7.1.0-centos7.tar.gz.tar HDP installation file. HDP-3.0.1.0-centos7-rpm.tar.gz.tar HDP-UTILS-1.1.0.22-centos7.tar.gz.tar HDF installation file. hdf-ambari-mpack-3.2.0.0-520.tar.gz.tar HDF-3.2.0.0-centos7-rpm.tar.gz.tar System PreparationBefore we start to install Ambari 2.7.1 cluster, some important steps we need to complete. Install some basic tools. 12yum -y install unzipyum -y install wget Change the Hostname on all cluster hosts. Edit the Host File on all cluster hosts. 1vi /etc/hosts Then append the content below to the Host File (Do NOT delete the original contents). 123192.168.110.210 master01.ambari.com master01192.168.110.211 slave01.ambari.com slave01192.168.110.212 slave02.ambari.com slave02 Set the Hostname on all cluster hosts. master01.ambari.com: 1hostname master01.ambari.com slave01.ambari.com: 1hostname slave01.ambari.com slave02.ambari.com: 1hostname slave02.ambari.com Edit the Network Configuration File on all cluster hosts. 1vi /etc/sysconfig/network Then append the contents below to the file. 12NETWORKING=yesHOSTNAME=&lt;fully.qualified.domain.name&gt; Test the hostname on all cluster hosts. 12hostnamehostname -f If the result above is incorrect, you need to repair it until the result is what you expect it. Set up Password-less SSH To have Ambari Server automatically install Ambari Agent on all your cluster hosts, you must set up a password-less SSH connection between the Ambari Server host and all other hosts in the cluster. Generate public and private SSH keys on the Ambari Server host. 1ssh-keygen Using the ssh command to copy a master public key file to slave nodes. 12ssh-copy-id -i ~/.ssh/id_rsa.pub root@slave01.ambari.comssh-copy-id -i ~/.ssh/id_rsa.pub root@slave02.ambari.com Test connections between the Ambari Server and the other cluster hosts. 123ssh root@master01.ambari.comssh root@slave01.ambari.comssh root@slave02.ambari.com It is successful when you do not need to input any passwords. Enable NTP on all cluster hosts. 12yum install -y ntpsystemctl enable ntpd Disable SELinux on all cluster hosts. 1vi /etc/selinux/config Then change the value of SELINUX. 1SELINUX=disabled Disable iptables on all cluster hosts. For Ambari to communicate during installation, certain ports must be open and available. So, the easiest way to do this is to temporarily disable iptables. 12systemctl disable firewalldsystemctl stop firewalld Install Oracle JDK8 (Not JRE) and add JCE extension package on all cluster hosts. You can also install JDK8 including JCE automatically on the step that runs command “amber-server setup” Download the Oracle JDK8 installation file from Oracle. Download the JCE installation file. 1http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html Install JDK8 firstly on all cluster hosts. 1yum install [your jdk8 yum installation file] Unzip JCE file to “$JAVA_HOME/jre/lib/security/“ on all cluster hosts. 1unzip -o -j -q jce_policy-8.zip -d /usr/java/jdk1.8.0_201-amd64/jre/lib/security Reboot. JDK HOME 1/usr/java/jdk1.8.0_201-amd64/ Install Mysql Server on the Ambari Server host. Download and add the repository, then update. 123wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum update Install mysql-server and start the service. 12yum install mysql-serversystemctl start mysqld Change the password of the user ROOT (the default password is blank). 1mysql -uroot -p 12set password for 'root'@'localhost' = password('root');set password for 'root'@'master01.ambari.com' = password('root'); Create the database “ambari” and the user “ambari”. 1234567create database ambari;create user 'ambari'@'localhost' identified by 'ambari';create user 'ambari'@'master01.ambari.com' identified by 'ambari';grant all on ambari.* to 'ambari'@'localhost';grant all on ambari.* to 'ambari'@'master01.ambari.com'; Download mysql-connector-java.jar and put this jar to a specific folder on the Ambari Server host. 12345678wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jarmkdir /usr/share/javacp /root/mysql-connector-java-5.1.47.jar /usr/share/java/#JDBC Driver Path/usr/share/java/mysql-connector-java-5.1.47.jar Setting up a local repository with NO Internet access on the Ambari Server host. Install and start httpd. 12yum -y install httpdsystemctl start httpd Untar installation file to the web server folder. 1234567mkdir /var/www/html/hdfmkdir /var/www/html/hdptar -xvf /root/ambari-2.7.1.0-centos7.tar.gz.tar -C /var/www/htmltar -xvf /root/HDP-3.0.1.0-centos7-rpm.tar.gz.tar -C /var/www/html/hdptar -xvf /root/HDP-UTILS-1.1.0.22-centos7.tar.gz.tar -C /var/www/html/hdptar -xvf /root/HDF-3.2.0.0-centos7-rpm.tar.gz.tar -C /var/www/html/hdf Confirm that you can browse to the newly created local repository. 1234567891011#Ambari BaseURLhttp://192.168.110.210/ambari/centos7/2.7.1.0-169/#HDP BaseURLhttp://192.168.110.210/hdp/HDP/centos7/3.0.1.0-187/#HDP-UTIL BaseURLhttp://192.168.110.210/hdp/HDP-UTILS/centos7/1.1.0.22/#HDF BaseURLhttp://192.168.110.210/hdf/HDF/centos7/3.2.0.0-520/ Downloading and Editing the Ambari Repository Configuration File to Use the Local Repository. 123wget -nv http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.1.0/ambari.repo -O /etc/yum.repos.d/ambari.repovi /etc/yum.repos.d/ambari.repo Replace the Ambari baseurl to your local repository. 123456789#VERSION_NUMBER=2.7.1.0-169[ambari-2.7.1.0]#json.url = http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.jsonname=ambari Version - ambari-2.7.1.0baseurl=http://192.168.110.210/ambari/centos7/2.7.1.0-169/gpgcheck=1gpgkey=http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.1.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1 Install and set up the Ambari server on the Ambari Server host. 1yum install ambari-server Then, start to set up ambari-server. 1ambari-server setup Run the SQL script. 12345mysql -uambari -puse ambari;source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql Start Ambari-server. 1ambari-server start Installing Ambari Cluster Browse Ambari WebUI, then use account admin to log in. Launch Install Wizard. Step 0: Name your cluster. Step 1: Edit BaseURL to your local repository. Step 2: Input a list of hosts using the hostname and input your ssh private key of the Ambari server host. Step 3: Confirm Hosts. Step 4: Choose Services. Step 5: Assign Masters. Step 6: Assign Slaves and Clients. Step 7-1: Input usernames and passwords for your services. Step 7-2: Create Databases and users for services, then input information for login. 123456789101112131415161718192021222324252627#Rangercreate database ranger;create user &apos;ranger&apos;@&apos;localhost&apos; identified by &apos;ranger&apos;;create user &apos;ranger&apos;@&apos;master01.ambari.com&apos; identified by &apos;ranger&apos;;grant all on ranger.* to &apos;ranger&apos;@&apos;localhost&apos;;grant all on ranger.* to &apos;ranger&apos;@&apos;master01.ambari.com&apos;;#RangerKMScreate database rangerkms;create user &apos;rangerkms&apos;@&apos;localhost&apos; identified by &apos;rangerkms&apos;;create user &apos;rangerkms&apos;@&apos;master01.ambari.com&apos; identified by &apos;rangerkms&apos;;grant all on rangerkms.* to &apos;rangerkms&apos;@&apos;localhost&apos;;grant all on rangerkms.* to &apos;rangerkms&apos;@&apos;master01.ambari.com&apos;;#Ooziecreate database oozie;create user &apos;oozie&apos;@&apos;localhost&apos; identified by &apos;oozie&apos;;create user &apos;oozie&apos;@&apos;master01.ambari.com&apos; identified by &apos;oozie&apos;;grant all on oozie.* to &apos;oozie&apos;@&apos;localhost&apos;;grant all on oozie.* to &apos;oozie&apos;@&apos;master01.ambari.com&apos;;#Hivecreate database hive;create user &apos;hive&apos;@&apos;localhost&apos; identified by &apos;hive&apos;;create user &apos;hive&apos;@&apos;master01.ambari.com&apos; identified by &apos;hive&apos;;grant all on hive.* to &apos;hive&apos;@&apos;localhost&apos;;grant all on hive.* to &apos;hive&apos;@&apos;master01.ambari.com&apos;; Step 7-3: Follow other steps to complete. Step 8: Review Step 9: Install, Start, and Test. Summary. Completion Installing HDF on Ambari 2.7.1 + HDP 3.0.1Regarding the HDF installation, I will write another post to describe specifically.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Meeting 401 Http Status Code when Visting Oozie UI by a browser in a Kerberos environment]]></title>
    <url>%2Fposts%2F3f9aa226.html</url>
    <content type="text"><![CDATA[Using your browser such as Firefox or Chrome to access your component services in Ambari with Kerberos Authorization may confront the error message above. The Http status code 401 showing that lacking the authorization to access specific resources. So solving this error is to assign specific authorization for the resource that you need to access. https://community.hortonworks.com/questions/212654/nifi-processor-connection-to-schema-registry-with.html Firefox Configuring Firefox Open Firefox and type the “about:config” in the address bar. Specify the hostname of your Ambari cluster to the properties(“network.negotiate-auth.delegation-uris” and “network.negotiate-auth.trusted-uris”) Typing Command “kinit” Assigning authorization to specific service which you want to access. 1kinit -kt [the file path of specific keytab file] [the principle of specific service] Other BrowsersAccessing the website to get concrete solutions for specific browsers. https://ping.force.com/Support/PingFederate/Integrations/How-to-configure-supported-browsers-for-Kerberos-NTLM]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Comparison between Hortonworks and Alibaba]]></title>
    <url>%2Fposts%2F720bb0c8.html</url>
    <content type="text"><![CDATA[公司 Hortonworks Alibaba 大数据产品 Ambari (提供Hadoop生态的管理、运维、升级等功能)HDP (提供大部分Hadoop相关开源组件)HDF (提供数据流相关的开源组件) Dataworks (大数据开发平台，提供在线编辑工具)Dataphin (新一代大数据平台，为阿里的数据中台业务中台铺路)QuickBI (提供全面的报表展示服务，为Martech助力) 市场占有率 国际市场的三驾马车之一（Cloudera, MapR），18年底完成与Cloudera公司的合并 国际市场占有率低（有一部分Flink的原因，Flink16年出现） 收费模式 100%开源，提供收费技术支持及培训 具体需要联系售前 产品特点 100%开源，使用不受任何限制 基础架构及上层服务均需要全套阿里产品，同时数据留存在阿里 技术栈 HadoopHDFSYarnSpark2(DataFrame + SQL)NifiKafkaKerberosZooKeeperRangerOozie…. MaxCompute(阿里大数据处理引擎，基于Hadoop早期版本定制)Blink(基于Flink的定制版，做batch及streaming处理) 主要开发语言 Java, Scala, SQL, 部分可视化拖拽（Nifi） 可视化拖拽, SQL 开发成本 高(需要部署开发环境，同时需要IDE开发工具) 低(配置+SQL，所有开发均在阿里在线开发环境完成) 运维成本 高(对Hadoop生态的优化需要自己完成) 低 第三方服务/系统对接 容易实现 需要看阿里是否支持 技术支持 有成熟的社区及非常完善的文档，基本上遇到的问题都能解决同时Ambari还提供智能支持的收费服务，也有收费培训等 具体暂不清楚，应该会有对应的技术工程师提供完备的服务，省去了花时间自己找解决方案 数据安全 Kerberos+Ranger 阿里产品 部署方式 基于虚机/Docker的部署，微软Azure提供深度支持 独立部署（需要有千万级数据的体量）多租户方式 (适于小体量同时价格便宜)]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cannot Start Namenode with Safemode Error]]></title>
    <url>%2Fposts%2Fab80d8c6.html</url>
    <content type="text"><![CDATA[It’s so annoying when you see the namenode error above, and always showing the same error after restarting several times. Fortunately, I found an answer online writen by a hortonworks staff. So, we don’t need to worry about this kind of error, and we can use the command below to ignore this error. 1hadoop dfsadmin -safemode leave Using the command below to get the state of the namenode 1hdfs dfsadmin -safemode get Incorrect steps for stopping your Ambari cluster may casue this kind of error.Following steps below can avoid. Clicking the button “Stop All”. Running the commands below. 123ambari-agent stop ambari-server stop Powering off VMs.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cannot start ambari services with 400 status code]]></title>
    <url>%2Fposts%2F7449623f.html</url>
    <content type="text"><![CDATA[Error MessageYou maybe meet this kind of error message showing below when you click the button “Start All”. Error message: java.lang.IllegalArgumentException: Invalid transition for servicecomponenthost, clusterName=ambari, clusterId=2, serviceName=SMARTSENSE, componentName=ACTIVITY_EXPLORER, hostname=ambari.com, currentState=STOPPING, newDesiredState=STARTED SolutionIt’s so annoying I cannot find any great solutions online until I saw this article in the hortonworks community. Step1: Update the state of components in Ambari So, update the table hostcomponentstate. Step2: Restart Ambari agents and server123ambari-agent restartambari-server restart ReasonStopping your Ambari cluster in an exceptional steps will cause this kind of message such as exceptional poweroff. So, we ought to follow the correct steps blow to avoid this kind of unexpectable error. Clicking the button “Stop All”. Running the commands below. 123ambari-agent stop ambari-server stop Powering off VMs.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kerberos Authentication Explained]]></title>
    <url>%2Fposts%2Fbfaac82a.html</url>
    <content type="text"><![CDATA[What is KerberosIn Greek mythology, Kerberos is a creature that is a three-headed dog that guards the gate toward the underworld. In a nutshell, it’s a security guardian.Back to our network world, Kerberos is a computer network authorization protocol that allows clients and servers to communicate in a secure manner (symmetric key cryptography). It is designed by MIT. TerminologyKerberos is a protocol which uses within the client-server model. Before we explore the authorization process, we ought to know some terms in Kerberos. KDC (Key Distribution Center) Which issues the TGT to the client and contains TGS and AS. AS (Authorization Server) Which forwards the username to a KDC. TGS (Ticket-granting Service/Server) A service that can generate TGT. TGT (Ticket-granting Ticket) A data segment that is time-stamped and encrypted by using the TGS secret key and returns to the client. principal A service node which guards by Kerberos. SS (Service Server) Also called a principal that provides specific services for clients. SPN (Service Principal Name) The literal meaning of this word. Realm Which encompasses all services that you can access. PrincipalWhen requiring access to a service or host, there are four components that you need to keep back in your mind. AS TGS SS Client So, let’s begin to cross this mysterious jungle. Process 1: Getting TGT The client sends the plaintext to AS. The plaintext contains: name/ID timestamp AS verifies timestamp, then lookups client by client’s username to ensure that the client is a legal principle. If the time gap between the client and AS is upper than 5min, the client will be refused by AS. AS generates a TGT and a TGS session key which used for communicating with TGS. AS sends two messages (TGT and TGS session key) back to the client. The client secret key is created by the client within the registration process (Using command “addprinc”). TGT which encrypted by TGS secret key (client cannot decrypt TGT) contains: your name/ID the TGS name/ID timestamp your network address lifetime of TGT TGT session key Info package which encrypted by client secret key which KDC owning it contains: TGS name/ID timestamp lifetime TGS session key The client fetches TGT and TGS session key which can be obtained by decrypting info package. Process 2: Requesting access to a specific service(principal)In this step, you just communicate with TGS. The client sends the plaintext request, the authenticator and TGT to TGS. The plaintext contains: service name/ID lifetime of the ticket for the service The authenticator which encrypted by TGS session key contains: your name/ID timestamp TGS verifies if the service exists. TGS decrypts TGT and the authenticator, then comparing properties. TGS can use the TGS secret key to decrypt TGT. So TGS can obtain the TGT session key which can decrypt the authenticator. After that TGS needs to compare the information which is TGT provided by AS to the information which is the authenticator created by the client. TGS sends the service session key and the service ticket. The service ticket which encrypted by service secret key contains: your name/ID service name/ID your network address timestamp lifetime of this service ticket service session key Info package which encrypted by TGS session key contains: service name/ID timestamp lifetime of the service ticket the service session key The client fetches the service ticket and the service session key which can be obtained by decrypting the info package. Process 3: Communicating with the serviceFrom now, you just communicate with the service. The client sends the authenticator and the service ticket to the service server.The authenticator encrypted by the service session key contains: your name/ID timestamp The service decrypts the service ticket and the authenticator, then compares properties.The service server can use the service secret key to decrypt the service ticket. So the service server can obtain the service session key which can decrypt the authenticator. After that, the service server needs to compare the information which is the service ticket provided by TGS to the information which is the authenticator created by the client. The service sends the authenticator to the client in order to confirm its identity.The authenticator encrypted by the service session key contains: the service name/ID timestamp The client decrypts the authenticator and knows it has been authenticated to use the service by cached the service ticket.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using Flume to transfer data from Kafka to HDFS in Ambari(2.4.2) with Kerberos]]></title>
    <url>%2Fposts%2Fdc7e5002.html</url>
    <content type="text"><![CDATA[Firstly, you don’t need to read the following post if your Flume upper than 1.5.2. You can complete your configuration file by following official documents.** Versions of components Ambari: 2.4.2 HDP: 2.4.3 HDFS: 2.7.1.2.4 Kafka: 0.9.0.2.4 Flume: 1.5.2.2.4 Unpredictable problemAssume that using Kafka to receive data from a topic, then store data to HDFS. It is obviously a good choice using Kafka source, memory channel, and HDFS sink. Nevertheless, it’s not easy like that after searching User Guide(1.5.2). After reading the contents above, it is pitiful that Kafka source not supporting Kerberos environment because of no properties for specifying principle and keytab file path. So why Flume 1.5.2 not supporting Kafka source? OK. Let’s check out the library within Flume (/user/hdp/2.4.3.0-227/flume/lib). There are some libs specifying Kafka(0.8.2). Well, it’s so weird that Ambari-2.4.2 using Kafka-0.9.0, but Flume-1.5.2 in Ambari-2.4.2 not using Kafka-0.9.0 instead of using Kafka-0.8.2. I don’t know why, but I also found similary problem after googling. Kafka starts to support Kerberos authorization after version 0.9. So it’s obvious that we can’t use Kafka source in Flume in Ambari-2.4.2 with Kerberos. From this moment, we got stuck by this annoying version of conflict. After complaining, we also need to solve this problem. But, how?Although Kafka-0.8.2 not supporting Kerberos, it ought to create a custom Kafka source. Appending Kerberos authorization code within your custom Kafka source that is a reasonable solution for this situation. Reasonable solutionCreating custom Kafka(0.9.0) sourceWriting your custom Kafka source which inheriting class AbstractSourceis just like a Kafka consumer. Attention: The dependencies of Kafka need to use the version of 0.9.0. Java codes paste below. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.johnny.flume.source.kafka;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.EventDeliveryException;import org.apache.flume.PollableSource;import org.apache.flume.conf.Configurable;import org.apache.flume.event.EventBuilder;import org.apache.flume.source.AbstractSource;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.nio.charset.Charset;import java.util.*;public class CustomKafkaSource extends AbstractSource implements Configurable, PollableSource &#123; private KafkaConsumer&lt;byte[], String&gt; consumer; @Override public Status process() throws EventDeliveryException &#123; List&lt;Event&gt; eventList = new ArrayList&lt;Event&gt;(); try &#123; ConsumerRecords&lt;byte[], String&gt; records = consumer.poll(1000); Event event; Map&lt;String, String&gt; header; for (ConsumerRecord&lt;byte[], String&gt; record : records) &#123; header = new HashMap&lt;String, String&gt;(); header.put("timestamp", String.valueOf(System.currentTimeMillis())); event = EventBuilder.withBody(record.value(), Charset.forName("UTF-8"), header); eventList.add(event); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return Status.BACKOFF; &#125; getChannelProcessor().processEventBatch(eventList); return Status.READY; &#125; @Override public void configure(Context context) &#123; Properties properties = new Properties(); properties.put("zookeeper.connect", context.getString("zk")); properties.put("group.id", context.getString("groupId")); properties.put("auto.offset.reset", "earliest"); properties.put("bootstrap.servers", context.getString("bootstrapServers")); properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT"); properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true"); this.consumer = new KafkaConsumer&lt;byte[], String&gt;(properties); this.consumer.subscribe(Arrays.asList(context.getString("topics").split(","))); &#125; @Override public synchronized void stop() &#123; super.stop(); &#125;&#125; Using the code “context.getString()” can get specific property value from your configuration file. Generating jar packageUtilizing your build automation tool - such as Maven, Gradle and etc - to generate your custom Kafka source jar. Replacing Kafka lib and Adding custom Kafka source jarIn this step, you ought to download a Kafka jar package (the version is 0.9.0), then copy the file “kafka-clients-0.9.0.0.jar” and the file that custom Kafka source jar to your lib path of Flume(/usr/hdp/2.4.3.0-227/flume/lib). After that, you also need to delete the old version of Kafka client jar(kafka-clients-0.8.2.0.jar). Other preparation for the final point Creating a Kafka topic and ensuring that it’s working. 12345678910111213kinit -kt /etc/security/keytabs/kafka.service.keytab kafka/bigdata.com@CONNEXT.COMcd /usr/hdp/2.4.3.0-227/kafka/bin/kafka-topics.sh --create --zookeeper bigdata.com:2181 --replication-factor 1 --partitions 1 --topic flume-kafkabin/kafka-topics.sh --list --zookeeper bigdata.com:2181bin/kafka-topics.sh --describe --zookeeper bigdata.com:2181 --topic flume-kafkabin/kafka-console-producer.sh --topic flume-kafka --broker-list bigdata.com:6667 --security-protocol SASL_PLAINTEXTbin/kafka-console-consumer.sh --topic flume-kafka --zookeeper bigdata.com:2181 --from-beginning --security-protocol SASL_PLAINTEXT Creating your storage folder in your HDFS for saving data. Editing your flume configuration fileFinally, it’s time to reach the peak. Just follow the configuration code below. 1234567891011121314151617181920212223242526272829amk.sources = kamk.sinks = hamk.channels = m#### Sources ####amk.sources.k.type = com.johnny.flume.source.kafka.CustomKafkaSourceamk.sources.k.zk = bigdata.com:2181amk.sources.k.groupId = flume-kafkaamk.sources.k.bootstrapServers = bigdata.com:6667amk.sources.k.topics = flume-kafka#### Sinks ####amk.sinks.h.type = hdfsamk.sinks.h.hdfs.fileType = DataStreamamk.sinks.h.hdfs.path = /johnny/flume/events/%y-%m-%damk.sinks.h.hdfs.filePrefix = eventsamk.sinks.h.hdfs.fileSuffix = .logamk.sinks.h.hdfs.round = trueamk.sinks.h.hdfs.roundValue = 10amk.sinks.h.hdfs.roundUnit = minuteamk.sinks.h.hdfs.useLocalTimeStamp = trueamk.sinks.h.hdfs.kerberosPrincipal = hdfs-bigdata@CONNEXT.COMamk.sinks.h.hdfs.kerberosKeytab = /etc/security/keytabs/hdfs.headless.keytab#### Channels ####amk.channels.m.type = memoryamk.sources.k.channels = mamk.sinks.h.channel = m Others need to noticeSimilarly, we also ought to write a custom Kafka sink in Flume(1.5.2) if we need it.The java code pastes below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.johnny.flume.sink.kafka;import org.apache.flume.*;import org.apache.flume.conf.Configurable;import org.apache.flume.sink.AbstractSink;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;public class CustomKafkaSink extends AbstractSink implements Configurable &#123; public KafkaProducer&lt;String, String&gt; producer; public String topic; @Override public Status process() throws EventDeliveryException &#123; Status status = null; Channel ch = getChannel(); Transaction txn = ch.getTransaction(); txn.begin(); try &#123; Event event = ch.take(); if (event == null) &#123; status = Status.BACKOFF; &#125; byte[] byte_message = event.getBody(); //生产者 ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(this.topic, new String(byte_message)); producer.send(record); txn.commit(); status = Status.READY; &#125; catch (Throwable t) &#123; txn.rollback(); status = Status.BACKOFF; if (t instanceof Error) &#123; throw (Error) t; &#125; &#125; finally &#123; txn.close(); &#125; return status; &#125; @Override public void configure(Context context) &#123; Properties properties = new Properties(); properties.put("bootstrap.servers", context.getString("bootstrapServers")); properties.put("metadata.broker.list", context.getString("brokerList")); properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); properties.put("serializer.class", "kafka.serializer.StringEncoder"); properties.put("request.required.acks", context.getString("acks")); properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT"); this.producer = new KafkaProducer&lt;String, String&gt;(properties); this.topic = context.getString("topic"); &#125;&#125; The related configuration file pastes below. 123456#### Sinks ####amk.sinks.k.type = com.johnny.flume.sink.kafka.CustomKafkaSinkamk.sinks.k.bootstrapServers = bigdata.com:6667amk.sinks.k.brokerList = bigdata.com:6667amk.sinks.k.acks = 1amk.sinks.k.topic = flume-kafka]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Kafka</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using Flume in Ambari with Kerberos]]></title>
    <url>%2Fposts%2Fa833198.html</url>
    <content type="text"><![CDATA[What is Flume Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. An Event is a unit of data, and events that carrying payloads flows from source to channel to sink. All above running in a flume agent that runs in a JVM. Three key components of Flume: Source The purpose of a source is to receive data from an external client and store it in a configured channel. Channel The channel is just like a bridge which receives data from a source and buffers them till they are consumed by sinks. Sink Sinks can consume data from a channel and deliver it to another destination. The destination of the sink might be another agent or storage. Attention:In HDP3.0, Ambari doesn’t use Flume continuously instead of using Nifi. Kerberos authorizationBefore we use flume, we need to configure it in Ambari with Kerberos. Visit Ambari UI and click the “Flume” service to change the configuration file. More importantly, you need to append security content at the end of the property “flume-env template” when you using Kafka components with Kerberos. Every Kafka client needs a JAAS file to get a TGT from TGS. 1export JAVA_OPTS=&quot;$JAVA_OPTS -Djava.security.auth.login.config=/home/flume/kafka-jaas.conf&quot; ConfigurationThere is a simple example that transferring log data from one pc to another pc, and I paste the configuration code below.Using avro-source and avro-sink is a perfect choice to transfer data from one agent to another agent. One PC configurationsource: exec-sourcechannel: memorysink: avro-sink 12345678910111213141516exec-memory-avro.sources = exec-sourceexec-memory-avro.sinks = avro-sinkexec-memory-avro.channels = memory-channel exec-memory-avro.sources.exec-source.type = execexec-memory-avro.sources.exec-source.command = tail -F /Users/JohnnyLiu/Documents/local_flume/data.logexec-memory-avro.sources.exec-source.shell = /bin/sh -c exec-memory-avro.sinks.avro-sink.type = avroexec-memory-avro.sinks.avro-sink.hostname = bigdata.comexec-memory-avro.sinks.avro-sink.port = 44444 exec-memory-avro.channels.memory-channel.type = memory exec-memory-avro.sources.exec-source.channels = memory-channelexec-memory-avro.sinks.avro-sink.channel = memory-channel Another PC configurationsource: avro-sourcechannel: memorysink: logger 1234567891011121314avro-memory-logger.sources = avro-sourceavro-memory-logger.sinks = logger-sinkavro-memory-logger.channels = memory-channelavro-memory-logger.sources.avro-source.type = avroavro-memory-logger.sources.avro-source.bind = bigdata.comavro-memory-logger.sources.avro-source.port = 44444avro-memory-logger.sinks.logger-sink.type = loggeravro-memory-logger.channels.memory-channel.type = memoryavro-memory-logger.sources.avro-source.channels = memory-channelavro-memory-logger.sinks.logger-sink.channel = memory-channel]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Kerberos</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Running Kafka consumer and producer in Kerberos Authorization]]></title>
    <url>%2Fposts%2F2d304b2.html</url>
    <content type="text"><![CDATA[No! No! No! Not this guy, we are talking about Apache Kafka. What is Kafka Kafka is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies. Feature: Scalability Fault-tolerant Multi-source Real-time streaming data Fast Versions of my Ambari components Ambari 2.7.1.0 HDP 3.0.1.0 HDF 3.2.0.0 PreparationUpdate configuration propertiesWhen you using Ambari to manage your Kafka, you don’t need to complete some annoying installation and configuration steps. But you still need to change or add some properties for Kerberos authorization. Change the property “listener” in the “Kafka Broker” tab. Because of using Kerberos, the protocol of the listeners must be updated. 1listeners = SASL_PLAINTEXT://localhost:6667 Update the protocol of brokers’ security. Change your brokers’ protocol (“security.inter.broker.protocol“) in the “Advanced Kafka-broker” tab. Add “KAFKA_OPTS” for the JAAS configuration file. After enabling Kerberos, Ambari sets up a JAAS (Java Authorization and Authorization Service) login configuration file for the Kafka client. Settings in this file will be used for any client (consumer, producer) that connects to a Kerberos-enabled Kafka cluster. You don’t need to change the content of the JAAS configuration file, you just need to add a command in the “kafka-env template” in the “Advanced Kafka-env” tab. 12export KAFKA_PLAIN_PARAMS=&quot;-Djava.security.auth.login.config=/usr/hdp/3.0.1.0-187/kafka/conf/kafka_jaas.conf&quot;export KAFKA_OPTS=&quot;$KAFKA_PLAIN_PARAMS $KAFKA_OPTS&quot; You definitely should change the location of your JAAS file. Create a Kafka topicIt is a key step that creating a Kafka topic before running a consumer and a producer.Excepting the topic creation command, the other beneficial topic commands shown below. Creating 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --create --zookeeper ambari.com:2181 --replication-factor 1 --partitions 1 --topic flume-kafka Listing all topics 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --list --zookeeper ambari.com:2181 Describing a specific topic 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --describe --zookeeper ambari.com:2181 --topic flume-kafka Running a Consumer and a Producer Initiating Kerberos authorization for Kafka client. 1kinit -kt /etc/security/keytabs/kafka.service.keytab kafka/hostname@REALM.COM Running a consumer 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-consumer.sh --topic flume-kafka --zookeeper ambari.com:2181 --from-beginning --security-protocol SASL_PLAINTEXT The security protocol must be specific. Running a producer 1/usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-producer.sh --topic flume-kafka --broker-list ambari.com:6667 --security-protocol SASL_PLAINTEXT The security protocol must be specific. So, now you can use the terminal tool to test your consumer and producer.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using spark-submit to submit a Spark application in Kerberos environment]]></title>
    <url>%2Fposts%2Fb0e85a85.html</url>
    <content type="text"><![CDATA[Spark-submit is a shell script that allows you to deploy a spark application for execution, kill or request status of spark applications. So, it is generally a depolyment tool for spark application. How spark runs an application Create SparkContext from driver. Request resources from Cluster Manager. CM find Worker Node, then response to spark driver. Spark driver connects to Worker Nodes directly, then communicate each other (such as sending tasks). So, we can notice that serveral components are ciratical - spark driver, CM, Worker Node - in this submission process. And spark can use different CM - pseudo cluster manager, standalone CM, yarn, mesos and kubernets - to request resources and schedule tasks for spark applications. Spark also provides 2 different types of deployment mode - client and cluster - for running spark driver which can creates a SparkContext. Cluster Manager typesCM which acquires resources on the cluster is a critical key componet for a distributed system. Pseudo cluster manager In this kind of non-distributed single-JVM deployment mode, Spark creates all the execution components - driver, excutor and master - in the same single JVM. Using this kind of cluster manager to test your spark application is very convient and easy. Run spark locally with two worker threads: 1--master local[2] Run spark locally with as many as logical cores on your machine: 1--master local[*] Standalone It is a simple cluster manager providing by Spark. We can use it as a development tool and a testing tool among the project. 1--master spark://hostname:port Hadoop Yarn The resources manager in Hadoop 2. I’s a excellent choice in ambari environment. 1--master yarn Apache Mesos A general cluster manager that can run Hadoop MapReduce and service application. Kubernetes (K8S) An open-source system for automating deployment, scaling, and management of containerized applications. Deployment mode types Client Spark driver runs where the job is submitted outside of the cluster. 1--deploy-mode client Cluster By contrast with clident deployment mode, spark driver runs inside the cluster. 1--deploy-mode cluster Steps for using spark-submit command to launch a spark application in ambari with kerberos authorization Get TGTBefore we using this tool, we need to get TGT for spark service. Using command kinit to get ticket. 1kinit -kt /etc/security/keytabs/spark.headless.keytab spark-bigdata@CONNEXT.COM Submit application Pseudo cluster manager Notices: The asterisk can be replaced by specific number that showing how many worker nodes to be applied. And the asterisk is just announced that acquiring worker nodes as many as logical cores on your machine. Specifying package path and your application class name to CM for informing entry point of your spark application. 1234 /usr/hdp/current/spark-client/bin/spark-submit \--master local[*] \--class com.johnny.demo.SparkDemo \/root/spark-demo-1.0.0.jar \ 2. Yarn-cluster **Notices:** * Specifying entry point of your spark application which just likes the mode of pseudo cluster manager. * Your application jar files and other files all need to upload to your cluster. Not local file system. 1234567 /usr/hdp/current/spark-client/bin/spark-submit \--class com.johnny.demo.SparkDemo \--master yarn \--deploy-mode cluster \--files /usr/hdp/current/spark-client/conf/hive-site.xml \--jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar \hdfs://bigdata.com:8020/johnny/oozie/workflow/shell-action/demo1/spark-demo-1.0.0.jar]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Two Ways of Connecting Hive For Developing or Testing]]></title>
    <url>%2Fposts%2F87dabd80.html</url>
    <content type="text"><![CDATA[Hive CLIHive CLI a legacy tool that had two main use cases. The first is that it served as a thick client for SQL on Hadoop. And the second is that it served as a command-line tool for Hiver Server. Hive Server had been deprecated and removed from Hive. So, it is not an ideal way to connect Hive by using Hive CLI. Command below for using Hive CLI. 1hive Beeline CLIIt is a client tool for connecting Hive Server2 through JDBC. Then you can use SQL to obtain or process data.Because of using JDBC, there are two tunnels for transmitting data. The first way is zookeeper which is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. The second way is the apache thrift server which is RPC framework for scalable cross-language service development. Before we use beeline, we need to get TGT from TGS for using hive service when your Hadoop clusters using Kerberos authorization. After that, you can use Hive directly. 1kinit -kt [your hive service keytab file path] [your hive service principal] You can checkout your kerberos.csv file to get a specific service keytab file path and service principal. Zookeeper 123beeline !connect jdbc:hive2://bigdata.com:2181/sephora;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/bigdata.com@CONNEXT.COM So, you maybe want to get your own JDBC connection URL by using zookeeper. You can visit your own Ambari UI, then click “[Services]-&gt;[Hive]”. In your hive dashboard, you can get your own URL. After this, we may be encounter a bug that needs to input username and password. So, just click “Enter” two times because of owning the authorization for hive service. https://issues.apache.org/jira/browse/HIVE-9144 Thrift Server 123beeline!connect jdbc:hive2://bigdata.com:10001/sephora;transportMode=http;httpPath=cliservice;principal=hive/bigdata.com@CONNEXT.COM We can get parameter values by visiting Ambari UI and checking the configuration of Hive. Just like using zookeeper, we also encounter that bug. So, just click “Enter” two times.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Enable Kerberos on Ambari]]></title>
    <url>%2Fposts%2F3f0454be.html</url>
    <content type="text"><![CDATA[Install a new MIT KDCInstall the KDC server Install a new version of the KDC server 1yum install -y krb5-server krb5-libs krb5-workstation Update the KDC server configuration file 1vi /etc/krb5.conf Update the name of realms and ensure the value of “renew_lifetime” is “7d” 1234567[libdefaults]default_realm = BIGDATA.COMdns_lookup_realm = falsedns_lookup_kdc = falseticket_lifetime = 24hrenew_lifetime = 7dforwardable = true Update the item “realms” 12345[realms]BIGDATA.COM = &#123; kdc = master1.bigdata.com admin_server = master1.bigdata.com&#125; Update the item “domain_realm” 123[domain_realm].bigdata.com = BIGDATA.COMbigdata.com = BIGDATA.COM Create the Kerberos database1kdb5_util create -s Start the KDC12/etc/rc.d/init.d/krb5kdc start/etc/rc.d/init.d/kadmin start Set up the KDC server to auto-start on boot. 12chkconfig krb5kdc onchkconfig kadmin on Create a Kerberos AdminYou need to create an admin account, then provide this admin credentials for enabling Kerberos on Ambari. Create a KDC admin 1kadmin.local -q "addprinc admin/admin" Ensure this admin account include the authorization to enter into the specific realms 1vi /var/kerberos/krb5kdc/kadm5.acl 1*/admin@BIGDATA.COM * Restart the kadmin process 1/etc/rc.d/init.d/kadmin restart Install the JCEIf you already use the JDK that contains the JCE, you ought to skip this step. Download the specific version of the JCE file For Oracle JDK 1.8: 1http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html For Oracle JDK 1.7: 1http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html Add the JCE file to the JDK installation direction 1$JAVA_HOME/jre/lib/security/ Enabling the Kerberos Enter into the configuration of the kerberos Choose the type of KDC Configure the Kerberos Install and test the Kerberos client Configure identitiesYou can just click the button “next”. Confirm ConfigurationYou need to download the CSV file that contains a list of the principals and keytabs. Stop services Kerberos cluster Start and test services]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Ambari</tag>
        <tag>Kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Recover a Crashed Mysql Database]]></title>
    <url>%2Fposts%2F4cf6882d.html</url>
    <content type="text"><![CDATA[Among the operation of Ambari, we may confront some annoying failure, such as a crashed MySQL database. So, we can run commands below to recover this kind of problem. Check the status of the crashed table. 1check table [table name] Repair crashed table. 1repaire table [table name] On the other hand, you can use commands below to repair whole crashed tables. 1mysqlcheck -uroot -p --repair --all-databases 1myisamchk --recover *.MYI]]></content>
      <categories>
        <category>Database</category>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Ambari</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tar command in Linux]]></title>
    <url>%2Fposts%2Fd36bfb53.html</url>
    <content type="text"><![CDATA[options:-c: create the archive-x: extract the archive-f: create or extract with the specific archive filename-t: displays or lists files in an archived file-v: display info-z: using gzip -C: extract files into a specific directory, not the current directory Creating an uncompressed tar archive. 1tar -cvf file.tar directory Creating a compressed tar archive by using gzip. 1tar -cvzf file.tar.gz directory Extracting files from archive. 1tar -xvf file.tar Extracting gzip tar archive. 1tar -xvzf file.tar.gz Extracting files into a specific directory. 1tar -xvf file.tar -C /var/html/ Displays or lists files in the archived file. 1tar -tvf file.tar]]></content>
      <categories>
        <category>Operation</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Basic Operations for User Account Management]]></title>
    <url>%2Fposts%2Ff4c81e09.html</url>
    <content type="text"><![CDATA[Install MySQL server (not MySQL package) 1yum -y install mysql-server Start and launch MySQL 123service mysqld startmysql -uroot -p Add users. 1CREATE USER 'user_name'@'host' IDENTIFIED BY 'password'; The value of host: %: connect to mysql server from any other machine localhost: just local host ip_address: specific ip address Grant authority to users. 1GRANT ALL PRIVILEGES on [db_name].[db_table_name] TO [mysql_user_name]@[the_value_of_host] IDENTIFIED BY '[mysql_user_password]'; [all privileges] can be set with specific authority(“select,delete,update,create,drop”) [db_name] can be set with all databases(“*”) [db_table_name] can be set with all tables(“*”) Update password. 1ALTER USER 'user_name'@'host' IDENTIFIED BY 'password'; Delete users. 1DROP USER 'user_name'@'host';]]></content>
      <categories>
        <category>Operation</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Auto-start Services on Boot in Linux]]></title>
    <url>%2Fposts%2F857f09d5.html</url>
    <content type="text"><![CDATA[CentOS6 Show the list of services that start on boot. 1chkconfig --list Add a service to auto-start on boot. 12chkconfig --add service_namechkconfig service_name on Confirm script is added successfully 1chkconfig --list service_name Disable an auto-start service. 12chkconfig service_name offchkconfig --del service_name CentOS7 Add a service to auto-start on boot. 1systemctl enable service_name Disable an auto-start service. 1systemctl disable service_name Check the status of auto-start service. 1systemctl status service_name]]></content>
      <categories>
        <category>Operation</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention for Using Markdown code block]]></title>
    <url>%2Fposts%2Fd2fb4425.html</url>
    <content type="text"><![CDATA[When you are using markdown code block, you may meet a strange circumstance that disorder within code blocks. Just like the image below. So, when you confront the same problem like me, you just need to delete your space symbols after your end of code block. If you put one more space after your code block, the system will not recognize that’s an end signal of the code block.]]></content>
      <categories>
        <category>Language</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>Hexo</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Step by Step Tutorial for Ambari Installation]]></title>
    <url>%2Fposts%2F7f0f012a.html</url>
    <content type="text"><![CDATA[System RequirementsAt first, we need to think about the setups of VMs. So, I list a table for VMs below. hostname ip operating system RAM disk space package master1.bigdata.com 192.168.110.150 centos6.5 64位 16g 50g ambari-servernamenode slave1.bigdata.com 192.168.110.151 centos6.5 64位 16g 50g ambari-agentdatanode slave2.bigdata.com 192.168.110.152 centos6.5 64位 16g 50g ambari-agentdatanode VM Deployment Login VMware vsphere client system Create 3 VMs with specific system requirements Set the name of VM Choose data storage Choose hardware configuration Load centos6 ios file into CD-ROM when bootingYou can use the minimum version of the centos. Then you need to click the option “Running system installation with system booting”. Configuration complete Operating system installation Power on the VM Choose the first one. Click “skip” to continue. Choose language, keyborad and timezone. Set root password. Click “Using entire drive” to continue. Choose “Write changes to disk” to continue. Reboot. VM Preparation1. Configuration and Installation for VMs(master and slave servers) Install java. 1yum -y install java Enable network adapter. 123vi /etc/sysconfig/network-scripts/ifcfg-eth0vi /etc/sysconfig/network-scripts/ifcfg-eth1vi /etc/sysconfig/network-scripts/ifcfg-eth2 Then, enable it to run after system booting 1ONBOOT=yes Using tool ping to check the status of accessing public network and local network. Change the system’s hostname on all servers. Update file(/etc/sysctl.conf), add a item 1kernel.domainname=master1.bigdata.com Update file(/etc/hosts), add doamins. 123192.168.110.150 master1.bigdata.com master1192.168.110.151 slave1.bigdata.com slave1192.168.110.152 slave2.bigdata.com slave2 Update file(/etc/sysconfig/network) 12NETWORKING=yesHOSTNAME=master1.bigdata.com Reboot systems Check the correction of hostname 12hostnamehostname -f Set up password-less SSH on all servers Disable SELinux 1vi /etc/selinux/config Set disabled for SELinux: 1SELINUX=disabled Enable access for using public key. Warning: Filename is sshd_config, not ssh_config. 1vi /etc/ssh/sshd_config Delete comment signal(“#”) blow: 123RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys Generate public key and private key on master server. 1ssh-keygen -t rsa Then, click the default option for configuration. Create directories and copy the public key to slave servers. 123slave$ mkdir ~/.sshslave$ cd ~/.sshslave$ touch authorized_keys Login to master server, then copy public key: 1maseter$ scp ~/.ssh/id_rsa.pub root@slave1:~/.ssh Login to slave server, then copy content from public key to a file(authorized_keys): 1slave$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys **Notice: You need to copy your content of local ssh public key to your local authorized_keys if you use standalone mode.** Install ntpd on all servers. Check to see if you have installed ntpd. 1rpm -qa | grep ntp If not, install ntpd 1yum -y install ntp Close transparent huge page on all servers. Append code block to file(/etc/rc.local) for closing transparent huge page. 1vi /etc/rc.local code block: 123456if test -f /sys/kernel/mm/transparent_hugepage/enabled; thenecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiif test -f /sys/kernel/mm/transparent_hugepage/defrag; thenecho never &gt; /sys/kernel/mm/transparent_hugepage/defragfi Reboot operating system. Check the status of the transparent huge page to ensure the status is “never”. 1cat /sys/kernel/mm/transparent_hugepage/enabled 2. Set up a local repository on master server. Install local repository tool. 1yum install yum-utils createrepo Install the http service. 1yum install httpd Start the http service. 1service httpd start Install tool wget. 1yum -y install wget Download Ambari &amp; HDP package files to the master server. 123wget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.2.0/ambari-2.4.2.0-centos6.tar.gzwget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6/HDP-UTILS-1.1.0.20-centos6.tar.gzwget http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.3.0/HDP-2.4.3.0-centos6-rpm.tar.gz Copy Ambari and HDP packages to http dir, then untar them. 12345cd /var/www/htmlmkdir hdptar -xvf ambari-2.4.2.0-centos6.tar.gz -C /var/www/htmltar -xvf HDP-2.4.3.0-centos6-rpm.tar.gz -C /var/www/html/hdptar -xvf HDP-UTILS-1.1.0.20-centos6.tar.gz -C /var/www/html/hdp Check repository address for next step. Ambari base URL and gpgkey: 12http://192.168.110.150/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ http://192.168.110.150/AMBARI-2.4.2.0/centos6/2.4.2.0-136/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins HDP base URL and gpgkey: 12http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins HDP-UTILS base URL and gpgkey(gpgkey file same with HDP gpgkey file): 12http://192.168.110.150/hdp/HDP-UTILS-1.1.0.20/repos/centos6http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins Copy repository config files. ambari repo file: 1wget -nv http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.3.0/ambari.repo -O /etc/yum.repos.d/ambari.repo hdp repo file: 1cp /var/www/html/hdp/HDP/centos6/2.x/updates/2.4.3.0/hdp.repo /etc/yum.repos.d/ Update repository config files. You ought to renew base URL and gpgkey URL in your ambari.repo and hdp.repo files. 3. Ambari-server setup on master server Install ambari server 1yum install /var/www/html/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ambari/ambari-server-2.4.2.0-136.x86_64.rpm Run command for ambari setup. 1ambari-server setup If you do not disable the SELinux, you need to press “y”. If your Ambari server under user root, you just need to press “n”. If you do not disable iptalbes, you need to press “y” to continue. Select the JDK version, then follow steps to install it. If you use a custom JDK, you will maybe meet some troubles that will happen within Ambari Installation. So, choosing option one for the recommendation. Choose database type You can choose embedded the database. If you choose MySQL, you need to install the MySQL database on the master server and input info for the Ambari setup. Then put a MySQL connector into a specific path and run the command below to specify the path of the connector. 1ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar Run ambari-server 1ambari-server start 4. Ambari-agent setup on slave servers. Copy ambari-agent rpm file to slave servers. 1scp /var/www/html/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ambari/ambari-agent-2.4.2.0-136.x86_64.rpm root@slave_server_ip:~ Install ambari-agent. 1yum install ~/ambari-agent-2.4.2.0-136.x86_64.rpm Renew ambari-server URL in config file. 1vi /etc/ambari-agent/ambari.ini Renew ambari-server URL below: 12[server]hostname = master1.bigdata.com Run ambari-agent. 1ambari-agent start InstallationBefore you launch the Ambari installation web UI, you ought to ensure things below. Start the service httpd. Stop the service iptables. Start the ambari-server on the master server and ambari-agent on slave servers. If you want to start quickly, it is advisable to choose default options. Open your browser, then access to the WebUI and click the item (“Launch Install Wizard”) URL: 192.168.110.150:8080 Account: admin Password: admin Get Started Select Version Choose local repository, then input base URL of HDP and HDP-UTILS. Install Options Input your master’s hostname and slaves’ hostnames. Then copy your master’s ssh private key into the item. Confirm Hosts Just click “ok” to continue. Choose Services Choose what service you want to add to your system. You can add new services to your system later on. Just click “ok” to continue. Assign Masters Assign Slaves and Clients Customize ServicesYou just need to input info for items with a red exclamation mark. Review Install, Start and TestIf you get to this step successfully, you can drink a cup of coffee and enjoy this time. SummarySo now, say hello world to Ambari.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using docker command to obtain dockers' IP]]></title>
    <url>%2Fposts%2Fde75b356.html</url>
    <content type="text"><![CDATA[check specific container’s ip address:1docker inspect -f '&#123;&#123;range.NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' your_container_name_or_id check all containers’ ip address:1docker inspect -f '&#123;&#123;.Name&#125;&#125; - &#123;&#123;range.NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' $(docker ps -aq)]]></content>
      <categories>
        <category>Operation</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Some terms in Big Data Ecosystem]]></title>
    <url>%2Fposts%2Faa930aca.html</url>
    <content type="text"><![CDATA[DMP(Data Management Platform)A platform for collecting and managing data for digital marketing purposes. DSP(Demand-side Platform)A platform that allows buyers of online advertising to manage ad exchange and pay to buy some advertising banners according to Real-time bidding. SSP(Supply-side Plateform)A platform that allows web publishers or digital media owners to manage their advertising banners and receive money from this platform. MDM(Master Data Management)A business method that helps companies or enterprises to integrate, manage, store critical data in a single point of reference.These data can provide for other business platforms or business systems(B2B, B2C, CRM, OMS, etc).And master data also are useful for marketing decisions and marketing analysis.In China, Alibaba announced a business term(“数据中台”) that is based on MDM.So, in the next few years, we need to build a digital smart platform to collect all sources of business data that can provideinformation for all kinds of business systems or platforms and bread down obstacles between business systems in companies or enterprises.]]></content>
      <categories>
        <category>Back-end</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>DMP</tag>
        <tag>MDM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Several Tricks for Using Hexo Efficiently]]></title>
    <url>%2Fposts%2Feacbb695.html</url>
    <content type="text"><![CDATA[change theme add a theme to your Hexo (eg. theme next) 1git clone https://github.com/theme-next/hexo-theme-next themes/next update your blog config file(_config.yml, not your theme config file) 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next add RSS plugin install RSS tools by npm 1npm install --save hexo-generator-feed update your blog config file(_config.yml, not your theme config file) 12345feed: # RSS订阅插件 type: atom path: atom.xml limit: 0plugins: hexo-generate-feed update your theme config file(theme/_config.yml) 1234# Set RSS to false to disable feed link.# Leave RSS as empty to use site's feed link, and install hexo-generator-feed: `npm install hexo-generator-feed --save`.# Set RSS to specific value if you have burned your feed already.rss: /atom.xml change the code block theme1234# Code Highlight theme# Available values: normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: night record the number of viewers12345678910# Show Views/Visitors of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzibusuanzi_count: enable: true total_visitors: true total_visitors_icon: user total_views: true total_views_icon: eye post_views: true post_views_icon: eye generate the excerptThere are two methods for generating excerpts for the home page. update theme config file (theme/next/_config.yml)It is not good for you when you are using markdown language because of the bad show effect. 12345# Automatically Excerpt. Not recommend.# Use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 300 add markdown code in your post to control excerpt accurately 1&lt;!-- more --&gt; add search plugin install search plugin 1npm install hexo-generator-search --save update the blog config file 12345search: path: search.xml field: post format: html limit: 10000 update the theme config file 1234567891011# Local search# Dependencies: https://github.com/theme-next/hexo-generator-searchdblocal_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1 # unescape html strings to the readable one unescape: false add share plugin get the module from GitHub 12cd themes/nextgit clone https://github.com/theme-next/theme-next-needmoreshare2 source/lib/needsharebutton update the theme config file 12345678910111213141516needmoreshare2: enable: true postbottom: enable: true options: iconStyle: box boxForm: horizontal position: bottomCenter networks: Weibo,Wechat,Twitter,Facebook float: enable: false options: iconStyle: box boxForm: horizontal position: middleRight networks: Weibo,Wechat,Twitter,Facebook add google analytics update the theme config file 12# Google Analyticsgoogle_analytics: your google track code add reward update the theme config file 123456789# Reward# If true, reward would be displayed in every article by default.# And you can show or hide one article specially through add page variable `reward: true/false`.reward: enable: true comment: Thanks For Your Donation! wechatpay: /images/wechatpay.jpg alipay: /images/alipay.jpg #bitcoin: /images/bitcoin.jpg generate a permanent post address install plugin 1npm install hexo-abbrlink --save update the blog config file 123456# permalink: :year/:month/:day/:title/# permalink_defaults:permalink: posts/:abbrlink.htmlabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex add comment module (valine) sign up for a leancloud account create an application for your comment system copy AppID and AppKey from your dashboard paste AppID and AppKey into your theme config file 123456789101112valine: enable: true # When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version. appid: xxxxxxxxxx # your leancloud application appid appkey: xxxxxxxxxx # your leancloud application appkey notify: false # mail notifier, See: https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: Just go go # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail # custom comment header pageSize: 10 # pagination size visitor: true # leancloud-counter-security is not supported for now. When a visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html comment_count: true # if false, comment count will only be displayed in post page, not in home page create classes in your leancloud account to store your data [not required] you can change your valine lanuage (themes/next/layout/_third-party/comments/valine.swig) 12345678910111213new Valine(&#123; el: '#comments', lang: 'en', verify: &#123;&#123; theme.valine.verify &#125;&#125;, notify: &#123;&#123; theme.valine.notify &#125;&#125;, appId: '&#123;&#123; theme.valine.appid &#125;&#125;', appKey: '&#123;&#123; theme.valine.appkey &#125;&#125;', placeholder: '&#123;&#123; theme.valine.placeholder &#125;&#125;', avatar: '&#123;&#123; theme.valine.avatar &#125;&#125;', meta: guest, pageSize: '&#123;&#123; theme.valine.pageSize &#125;&#125;' || 10, visitor: &#123;&#123; theme.valine.visitor &#125;&#125; &#125;); [not required] you can also add a white list for security use image plugin install “hexo-asset-image” plugin 1npm install hexo-asset-image --save update your blog config file (_config.yml) 1post_asset_folder: true if you use the abbrlink plugin to change your permalink attribute in your blog config file, you need to edit this plugin to fit your config (node_modules/hexo-asset-image/index.js). Then you add a code block into this javascript file. 123var link = data.permalink; link = link.replace('.html', '/'); //新增加，针对修改后的permalinkvar beginPos = getPosition(link, '/', 3) + 1; Ok, you can generate your new static HTML file. 12hexo cleanhexo g add creative commons module to your blogIt is convenient and useful for a bloger to announce your blog copyright announcement. So, you can add this below to your posts or add this to your profile information page. update your theme config file 1234567# Creative Commons 4.0 International License.# https://creativecommons.org/share-your-work/licensing-types-examples# Available values: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zerocreative_commons: license: by-nc-sa sidebar: true post: true]]></content>
      <categories>
        <category>Front-end</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
</search>
